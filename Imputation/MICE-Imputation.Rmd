---
title: "ID5059 - Imputation"
output:
  html_document:
    df_print: paged
  pdf_document:
    latex_engine: xelatex
---

## Python and R Hybrid

How can we use both?
The `reticulate` library (2020) enables us to use both Python and R functions and coding in one place, a R Markdown file.

More information on the library specifications and functionalities can be found [here](https://rstudio.github.io/reticulate/)

``` {r, setup, echo = TRUE, message = FALSE, warning=FALSE}

library(reticulate) # Allowing to use Python code in RMarkwdown.

```

``` {r, echo = FALSE, message = FALSE, warning=FALSE}

# Source all functions from the python script `Imputation_Simulation_Study()`, the relevant functions are shown in the markdown below as well.
source_python("Imputation_Simulation_Study.py") # Importing python functions used in final simulation.

```

All other requirements for this document:

``` {r echo = TRUE, message = FALSE, warning=FALSE}

# For general use
library(dplyr)         # Allows to use '%>%' and handy functions that help with data wrnagling.
library(tidyr)         # Allows to use method 'gather()'

# For Analysis
library(missForest)    # For multivariate methods imputers
library(mice)          # For multivariate methods imputers

# For Performance Metrics
library(caret)         # For confusion matrix
library(matrixStats)   # For calculating sd of multiple columns

```

---

# Conducting Simulation Study of Imputation Methods

## References

**References for this document**

[1 : Flexible Imputation of Missing Data - Second Edition - Stef Van Buuren](https://stefvanbuuren.name/fimd/)

[2 : Inference and Missing Data - Rubin](https://www.jstor.org/stable/2335739?seq=1)

[3 : MICE: Multivariate Imputation by Chained Equations in R](https://www.researchgate.net/publication/44203418_MICE_Multivariate_Imputation_by_Chained_Equations_in_R)

[4 : MissForestâ€”non-parametric missing value imputation for mixed-type data](https://academic.oup.com/bioinformatics/article/28/1/112/219101)

[5 : Imputing Missing Data with R MICE Package](https://datascienceplus.com/imputing-missing-data-with-r-mice-package/)

[6 : MissForest : Non-Parametric imputation technique](https://arxiv.org/pdf/1105.0828.pdf)

## Imputation

> Imputation is the act of converting an incomplete sample into a complete sample. [1]

The idea behind this analysis is to construct a simulation study. This traditionally can be split into three key steps:

1. Simulation Step
2. Application of Methods
3. Assessing Performance of Methods

----

### 1. **Removing Data At Random** [Simulation Step]

#### Types of Missing Data

Rubin [2] classified missing data problems into three categories, which are all driven by the so-called **response mechanism**.
This mechanism can be modelled, as long as an assumption is made as to which response mechanism drove the data to be missing in the first place. The categories are **Missing Completely at Random** (MCAR), **Missing at Random** (MAR), **Missing not at Random** (MNAR). In short, these can be described as;

1. **MCAR** : when the probability of data missing is the same across the entire datset.
2. **MAR** : when the probability of data missing is the same across groups within the dataset.
3. **MNAR** : when the probability of data missing depends on an unknown factor.

The current **methodologies** in the literature rely on a **key assumption** choice of either three categories. Hence, certain methods will be more suitable than others depending on the data and whether the assumption is met or not will underpin the validity of statistical inferences.

Another important distinction to make it between **Univariate** and **Multivariate Missing Data**. These are defined as the problem where only one feature is missing and the problem where more than one feature is missing respectively. In our simulation function we will have freedom to choose every time which case to be in.

#### In Practice

For this special case, the simulation step is slightly unusual. Normally one would sample from a known distribution, however in this instance the sampling happens as way to **identify the location of the values which will be dropped**, i.e. become NaN.

1. For the **MCAR** Case:

  Mathematically, let $n$ be the total number of observations (rows) in the dataset and $m$ be the total number of features (columns) in the dataset. Hence, let $U_1$ and $U_2$ be two sets $U_1 \in {1,...,n} \text{  and } U_2 \in {1,...,m}$. By sampling with replacement k times from these two sets independently, such that any outcome is equally likely, the resulting ordered outcomes indicate the indexing of the element which will be dropped. These indexes can be viewed as the tuples $(u_1{_i}, u_2{_i}) \text{  for  } i \in {1,...,k}$.

##### Function to Remove Data (in Python)

``` {python eval = FALSE, message = FALSE, warning=FALSE}

def remove_random_values(true_df, columns, percentage):
    """
    Funtion the will remove values from the selected columns at random,
    then replace the removed values with np.nan which is NaN.

    PSEUDOCODE:

    1. Random sample row indexes (with replacement)
    2. Random sample column labels (with replacement)
    3. Extract random entry using 1. and 2. and replace with np.nan
    4. Repeat (1-3) until total proportion of data has been extracted

    :param df: pandas DataFrame which is the full data
    :param columns: list of columns to 'drop' values from. Note: must be string name of columns
    :param percentage: float input such as 0.20 (i.e. 20%)
    :return: pandas DataFrame of the same shape as the original with NaN entries
    """

    # Set up: Focus on subset of data dependent on columns,
    #         hence extract information about subset and number of values to drop
    subset = true_df[columns].copy()
    nrow, ncol = true_df[columns].shape
    n_samples = int(percentage*nrow)

    # 1. Pick out a vector of random samples (row indexes)
    row_indexes = np.arange(0, nrow, 1).tolist()

    # 2. Sample with replacement from the list of possible indexes
    #    note, `choices` is used to sample with replacement
    sampled_row_indexes = random.choices(row_indexes, k = n_samples)
    sampled_columns = random.choices(columns, k = n_samples)

    # 3. Extract entry and replace accordingly 
    for i in range(0, n_samples):
        if sampled_columns[i][0:3] == 'cat':
            subset.loc[sampled_row_indexes[i], sampled_columns[i]] = 'nan'
        else:
            subset.loc[sampled_row_indexes[i], sampled_columns[i]] = -1
            # this is the convention for a number of sklearn packages

    # Notes: having it as a loop ensures we are not holding a massive dataframe in memory so its a lot faster.
    # Notes: must use 'nan' and numerical as np.nan would not be updated in step 4. otherwise.

    # 4. Update data frame with new NaN values
    true_df.update(subset)
    true_df = true_df.replace('nan', np.nan)
    true_df = true_df.replace(-1, np.nan)

    return(true_df)

```

----

### 2. **Choosing the Imputation Methods** [Methods Step]

#### Types of Models

The simplest and most common solutions for missing data problems are 

* *likewise deletion*, which is simply removing the rows with missing elements from the dataset,
* *mean* or *median imputation*, which is substituting the missing feature with the sample mean or median over all observed values in that same feature column.
* *regression imputation*, which is about creating a regression model, hence incorporating the effects of other features, and using the model to predict the missing data.
* *stochastic regression imputation*, which is similar as above but incorporates variability of the predictions.
* *and more...*

In general however, these fall into two broad categories: **Univariate** and **Multivariate Imputation Methods**.
This simply indicates whether the method takes into account all the data features to determine the missing data (multivariate) or only the column the missing data belongs to (univariate). 

There is a key increase in **computational effort when using multivariate methods**, however these often perform better as they take into consideration the relationship between all the data when making a choice. This implies that often **correlation structures** across features are kept intact, compared to univariate methods which completely disregard them. 

For the sake of this analysis and simulation study, we will explore both in some level of detail.

Due by **multivariate nature of the sampling** *(i.e. dropping data from more than one feature)*, there are three main strategies to consider:

1. **Monotone data imputation**. Imputations are created by a sequence of univariate methods;
2. **Joint modeling**. Imputations are drawn from a multivariate model fitted to the data;
3. **Fully conditional specification**, also known as multivariate imputation by chained equations. A multivariate model is implicitly specified by a *set of conditional univariate models*. Imputations are created by drawing from iterated conditional models.

    **Note** : 1. is better suited for monotone missing data patterns and 2. and 3. are suitable for general missing data patterns.

#### **Univariate Methods** to consider:

Univariate imputation methods chosen fall into some of the standard ideas which were mentioned before.

* `Median SimpleImputer`, this is a standard sklearn imputer which takes the column feature and uses the median value as the imputed value all missing values. *Note*: this only copes with numerical features.

* `MostFrequent SimpleImputer`, this is another standard sklearn imputer which takes the column feature and uses the most frequent category as the imputed value for all missing values. *Note*: this only copes with categorical features.

##### Function to Run Univariate Imputation Methods

``` {python eval = FALSE, message = FALSE, warning=FALSE}

def univariate_imputation_method(nans_df):
    """
    Function which runs through our chosen three methods: Median Imputation, and Most Frequent Imputation
    
    Code from https://dzone.com/articles/imputing-missing-data-using-sklearn-simpleimputer helped remove errors
    when imputing isolated columns alone. Was getting error 'expected 2D array and was provided with 1D array'

    :param nans_df: pandas DataFrame which has missing data - subset on the columns to impute only
    :return: the imputed dataset
    """

    # Dataset that will be imputed and returned to main
    imputed_data = nans_df.copy()

    # Looping through each column that has to be imputed.
    # Doing this tactic to capture name of column
    for column in imputed_data:
        # Checking if the column is categorical data type and using most frequent tactic
        if column[0:3] == 'cat':
            imputer_tactic = SimpleImputer(strategy = "most_frequent")
        # Checking if the column is numerical data type and using mean tactic
        else:
            imputer_tactic = SimpleImputer(strategy = "mean")
        
        # Imputing the specific column with the appropriate specific tactic
        imputed_data[column] = imputer_tactic.fit_transform(imputed_data[column].values.reshape(-1,1))[:,0]

    return imputed_data

```

#### **Multivariate Methods** to consider:

For this analysis we focused on two key approaches.

* `MICE`, Multivariate Imputation by Chained Equations. More info can be found in the [docs](https://www.rdocumentation.org/packages/mice/versions/3.13.0/topics/mice)

* `MissForest` Imputer, this is a Nonparametric Missing Value Imputation using **Random Forest**. More info can be found in the [docs](https://cran.r-project.org/web/packages/missForest/missForest.pdf)
   
    **Note** : both cope with mixed data well.

``` {python message = FALSE, eval = TRUE, echo = FALSE, warning=FALSE}

## FUNCTION
#  Function that reads the true data frame we want to apply imputation research on.
#  Easier to construct a function for it so we can call it directly from R
## INPUTS
#  full_path: string input representing path to the full training data
## OUTPUT
#  data frame containing all true values.
def read_true_values(full_path):
  true_df = pd.read_csv(full_path)
  return(true_df)
  
```

##### MICE Fitting and Function

One of the key assumptions of MICE is that the **response mechanism is MAR**. Hence, that there is value in imputing values using a combination of all other features of the data. Which features are used as the *predictive variables* for the *imputed values* is shown in the output of PredictiveMatrix.

> For datasets containing hundreds or thousands of variables, using all predictors may not be feasible (because of multicollinearity and computational problems) to include all these variables. It is also not necessary. [1]

The **recommended steps for large datasets**, in short, are:

1. Include all features that appear in the model that will be applied to the data after imputation, including the outcome.
2. Include any feature which is known to be likely to be a driver for the missing data.
3. Include any feature which seems to explain a lot of the variance of the data.

**Note** : The mice package contains several tools that aid in automatic predictor selection. The `quickpred()` function is a quick way to define the predictor matrix using the strategy outlined above. [3] ...  The mice() function detects multicollinearity, and solves the problem by removing one or more predictors for the model. Each removal is noted in the loggedEvents element of the mids object.

Lastly, there is a wide range of choices in terms of the `methods` to deploy for this multivarite fitting.
More information can be found at [1] - *Section 6.3 Model form and predictors*.

The **default methods used** are:

* for unordered factor variables with more than two levels : `polyreg` - Bayesian Polytomous Regression
* for continuous variables : `ppm` - Predictive Mean Matching

``` {r message = FALSE, eval = TRUE, warning = FALSE}

# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

### FUNCTION
# Function that imputes missing values using the 'mice' machine learning algorithm.
## INPUT
# nans_df: data frame containing all NA values.
# columns: which columns to be considered in the imputation prediction.
## OUTPUT
# (data frame's) that data was imputed with mice. According to the variable m is the number of data frames.
impute_using_mice <- function(nans_df, columns) {
  
  imp_df_mice <- mice(nans_df[columns], m = 3, maxit = 5, print = FALSE)
  
  return(imp_df_mice)
  
}

# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

```

**Notes**;

* Interestingly, when running the MICE imputer on the categorical data only, this error comes up:

  `# Error in edit.setup(data, setup, ...) : mice detected constant and/or collinear variables. No predictors were left after their removal`

  Which indicates to me that all categorical variables may be drawn from the exact same sampling and that the labelling has no true meaning.
  Hence, for some meaningful predictions I kept a continuous feature `cont0` as the main predictor for both categorical features to impute.

##### MissForest Fitting and Function

The machine learning imputation method 'MissForest' follows an iterative lifecycle and is **based on the 'Random Forest' predictive model**. The specific algorithm does not fill missing cells with constants but with predictions. It predicts the missing values using a 'Random Forest' classification or regression model and the final predicted cell value is the **average or the majority of predictions obtained**.

 * Average used when we filling numerical data types.
 * Majority is used when we filling categorical data types.

The steps and process of a **MissForest lifecycle**:

 1. Fill missing values using the mode ('most frequent') or mean of the column.
 2. Train a Random Forest model using a complete data set.
 3. Predict the missing values using the model trained above.
 4. Re-fit Random Forest with dataset filled with the previous predicts values rather than mean/mode
 5. Predict missing values again.
 6. The above process is done in an iterative fashion and stops according to the stopping technique used
 
The MissForest imputation technique has outperformed other methods multiple times in different researches. Using the Random Forest models allows to take the advantage of out of **bag sample evaluating technique**, which does not need a test set. These techniques help and allow the algorithm to **perform very well when the data follows a complex non linear relation** within it. However when it follows a linear relation, other models are preferred which are created to capture such relations. Another benefit of using the MissForest it that it considers the randomness of data and is computationally efficient, which allows to deal with data of high dimensions. In addition to, the imputation method can deal with categorical and numerical data types. When predicting a categorical data type,  classification tree models are used in the forest. On the other hand when predicting a numerical data type, regression tree models are used in the forest.

The main benefit of using a MissForest model is that the algorithm is non-parametric. It does not assume any assumptions on the structural aspects of the data. This has no constraint of when a MissForest is appropriate, as it can deal with structured data and unstructured data. More on the MissForest algorithm can be found on reference [6] 


We can apply the MissForest imputation method using R package 'missForest' provided for us.

``` {r message = FALSE, eval = TRUE, warning = FALSE}

# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

### FUNCTION
# Function that imputes missing values using the 'missForest' machine learning algorithm.
## INPUT
# nans_df: data frame containing all NA values.
# columns: which columns to be considered in the imputation prediction.
## OUTPUT
# (data frame) that data was imputed with missForest
impute_using_missForest <- function(nans_df, columns) {
  
  imp_df_forest <- missForest(nans_df[columns], maxiter = 5)$ximp
  
  return(imp_df_forest)
  
}

# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

```

**Notes**;

* By construction of algorithm cannot run on categorical variables with more than 53 factors: drop cat10 and cat8 and cat5. There are different techniques to deal with this phenomeno. We can apply a One v One Strategy but this would make the model more computationl expensive. Since we have many multiple variables, we choose to not include them in the Random Forest and use the rest. 

----

### 3. **Evaluating Performance of the Imputation Methods** [Evaluation Step]:

#### Types of Adequate Performance Metrics:

##### For Continuous Features:

* **Normalized Root Mean Squared Error (NRMSE)**:

  $\sqrt{mean((X_{true} âˆ’ X_{imp})^2)/var(X_{true})}$

  where
    $X_{true}$ the complete data matrix and $X_{imp}$ the imputed data matrix
    
  *For good performance we expect values to be close to zero.*
    
* **Difference between true value and imputed value**

  $D = x_{true} - x_{imp}$
  
  where
    $x_{true}$ the true value from the data matrix and $x_{imp}$ the imputed value from the data matrix
    
  *For good performance we expect values to be concentrated about zero.*
    
* **Percentage difference between true value and imputed value**

  $PD = 100 * |(x_{true} - x_{imp})/x_{true}|$
  
  where
    $x_{true}$ the true value from the data matrix and $x_{imp}$ the imputed value from the data matrix
  
  *For good performance we expect values to be concentrated close to zero.*

##### For Discrete Features:

* **Confusion Matrix** : count the number of *True Positives*, *False Negatives*, *False Positives* and *True Negatives* and store them in a matrix to assess performance of predictor.

    *Good performance is indicated by higher counts in the diagonal of the matrix.*

* **Accuracy** : sum the counts in the *diagonal* of the confusion matrix (True Positive and True Negative) and divide by the sum of all entries in the confusion matrix.

    *Good performance leads to a value close to 1 and bad performance to a value around 0.*

##### Functions to Evaluate Performance Metrics (Python)

``` {python message = FALSE, eval = FALSE, warning=FALSE}

def performance_results_py(true_df, imp_df, nans_df, columns):
    """
    Function to extract true values and imputed values and compute performance metrics.
    :param true_df: pandas DataFrame containing true values
    :param imp_df: pandas DataFrame containing imputed values
    :param nans_df: pandas DataFrame containing nans
    :param columns: columns containing imputed values
    :return: list of results such that indexing returns results for each column (in columns) respectively
             for each column you then have the output from performance_metrics() which depends on the data type
    """

    results = []
    for column in columns:

        # a. Extract boolean vector (True/False) to identify Imputed True Data Vectors
        # Expect different values in cell, because comparing true with the NaN removed.
        boolean_values = (true_df != nans_df)[column].to_numpy().tolist()
       
        # c. Extract Imputed Data Vectors
        # Using Boolean Pointer list from above, extract true values
        true_values = true_df[column][boolean_values]
        # Using Boolean Pointer list from above, extract imputed values
        imputed_values = imp_df[column][boolean_values].to_numpy().tolist()

        # d. Evaluate performance metrics
        # Call function the two columns and obtain metrics. 
        results.append(performance_metrics(imputed_values, true_values))

    return(results)

def performance_metrics(imputed_values, true_values):
    """
    Function to calculate all performance metrics of interest for each list of imputed vs true values.

    :param imputed_values: list of imputed values
    :param true_values: list of true values
    :return: if categorical - measures of accuracy and the confusion matrix
             if numerical - measures of raw difference and percentage difference between
                            imputed value and true value
             
    Note: each list for now is specific to each column, as it supports univariate imputations.
    Note: depending the on the methods used the columns are independent of each other,
          e.g. the median imputation does not use any other column at all -
          implying that we can assess performance independently of one another.
    """

    # If Categorical Feature
    if isinstance(imputed_values[0], str):

        # Confusion Matrix
        conf_matrix = metrics.confusion_matrix(true_values, imputed_values)

        # Accuracy
        accuracy = metrics.accuracy_score(true_values, imputed_values)

        return(conf_matrix, accuracy)

    # If Numerical Feature
    elif isinstance(imputed_values[0], float):

        # 1. The difference between the imputed value and truth
        D = np.array(imputed_values) - np.array(true_values)
        sample_mean = np.mean(D)
        sample_sd = np.sqrt(np.var(D))
        
        # 2. The percentage difference between the imputed value and truth
        PD = 100 * abs((np.array(imputed_values) - np.array(true_values))/np.array(true_values))
        
        # 3. Normalised Root Mean Squared Error
        NRMSE = sqrt(mean_squared_error(true_values, imputed_values)/np.var(true_values))

        return(D, PD, sample_mean, sample_sd, NRMSE)

    else:

        return(None)
        
        
```

##### Function to Evaluate Performance Metrics (R)

``` {r message = FALSE, warning=FALSE}

# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

## FUNCTION
#  Function to extract true values and imputed values and compute performance metrics for mice.
## INPUTS
#  true_df: data.frame containing true values
#  imp_df: output directly from methods `mice()` 
#  columns: vector input containing strings representing column names which contain imputed values
## OUTPUT
#  list containing:
#    for categorical - measures of accuracy and the confusion matrix
#    for numerical - measures of raw difference and percentage difference between
#                   imputed value and true value
## NOTE
# number_iterations : Number of iterations mice has done.
# Each iteration computes a different imputation, we will find results from all imputations and get their average values.
performance_results_mice <- function(true_df, imp_df, columns) {
  
  # Set up
  
  number_iterations <- ncol(imp_df$imp[[columns[1]]])
  all_results <- list()

  # For each column we imputed values we will found a metric.
  for (column in columns) {
  
    # 1. Extract True and Imputed
    
    # a) extract row indexes containing imputed values
    imp_rows <- row.names(imp_df$imp[[column]])
    
    # b) use row indexes to find true values
    truth <- data.frame(true_df[[column]])[imp_rows, ]
    
    # 2. Check if it is continuous or categorical values to apply appropriate computations.
    
    # a) If categorical column
    if (startsWith(column, "cat")) {
      
      accuracies <- c()
      
      for (i in 1:number_iterations) {
        # extract imputed values for each iteration of mice.
        pred <- imp_df$imp[[column]][[i]]
        # compute metrics
        analysis_table <- table(pred, truth)
        res <- confusionMatrix(analysis_table)
        accuracies <- c(accuracies, res$overall[1])
      }
      
    all_results$conf_matrix[[column]] <- res$table
    all_results$accuracy[[column]] <- mean(accuracies)
    
    # b) If numerical column
    } else {
      
    all_results$D[[column]] <- truth - imp_df$imp[[column]][,]
    all_results$PD[[column]] <- 100 * abs((truth - imp_df$imp[[column]][,]) / truth)
    all_results$mean[[column]] <- colMeans(all_results$D[[column]])
    all_results$sd[[column]] <- colVars(as.matrix(all_results$D[[column]]))
    
    nrmse <- c()
    for (i in 1:number_iterations) {
      # extract imputed values for each iteration of mice.
      pred <- imp_df$imp[[column]][[i]]
      # compute metrics
      nrmse <- c(nrmse, sqrt((RMSE(pred, truth))^2/var(truth)))
    }
    
    all_results$nrmse[[column]] <- mean(nrmse)

    }
  }
  
  return(all_results)
  
}

# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

## FUNCTION
#  Function to extract true values and imputed values and compute performance metrics for missForest.
## INPUTS
#  true_df: data.frame containing true values
#  imp_df: output directly from methods `missForest().ximp`
#  nans_df: data.frame which we randomly replaced true values with NA's
#  columns: vector input containing strings representing column names which contain imputed values
## OUTPUT
#  list containing:
#    for categorical - measures of accuracy and the confusion matrix
#    for numerical - measures of raw difference and percentage difference between
performance_results_missForest <- function(true_df, imp_df, nans_df, columns) {
  
  all_results <- list()

  # For each column we imputed values we will find a metric.
  for (column in columns) {
    
    # 1. Extract True and Imputed
    
    # a) extract true values to compare with imputed.
    rows_imputed <- nans_df %>%
      mutate(row_names = row.names(nans_df)) # Creating a row_id in data frame.
    
    # b) Keeps rows that have NA values in column only.
    rows_imputed <- rows_imputed[is.na(rows_imputed[[column]]), ] 
    
    # c) Extract the cells for true and imputed values to compare between.
    truth <- data.frame(true_df[[column]])[rows_imputed[["row_names"]], ]
    pred <- data.frame(imp_df[[column]])[rows_imputed[["row_names"]], ]
    
    # 2. Check if it is continuous or categorical values to apply appropriate metrics.
    
    # a) If categorical column
    if (startsWith(column, "cat")) {
      
      accuracies <- c()

      analysis_table <- table(pred, truth)
      res <- confusionMatrix(analysis_table)
      accuracies <- c(accuracies, res$overall[1])
      
      all_results$conf_matrix[[column]] <- res$table
      all_results$accuracy[[column]] <- mean(accuracies)
      
    # b) If numerical column
    } else {
      
    all_results$D[[column]] <- truth - pred
    all_results$PD[[column]] <- 100 * abs((truth - pred) / truth)
    all_results$mean[[column]] <- mean(all_results$D[[column]])
    all_results$sd[[column]] <- sd(as.matrix(all_results$D[[column]]))
    all_results$nrmse[[column]] <- sqrt((RMSE(pred, truth))^2/var(truth))
    
    }
  }

  return(all_results)
  
}

# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

```

##### Function to Store Performance Metrics (R)

```{r}

# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

## FUNCTION
#  Function to extract all results for statistical testing in one dataframe
## INPUTS
#  simple_imputer_result: results from fitting
#  mice_result: results from fitting
#  missforest_result: results from fitting
#  columns: vector input containing strings representing column names which contain imputed values
## OUTPUT
#  dataframe
store_performance_results <- function(simple_imputer_result, mice_result, missforest_result, columns) {
  
  final_results <- c()
  
  i = 0 
  # Loop through each column to store respective results of interest
  for (column in columns) {
    i = 1 + i
    
    if (startsWith(column, "cat")) {
      # - - - - For Categorical Columns
      
      accuracy <- c(simple_imputer_result[[i]]$accuracy, mice_result$accuracy[[i]], missforest_result$accuracy[[i]])

      accuracy_results <- data.frame("performance" = accuracy,
                                     "performance_type" = rep("accuracy", each = 3),
                                     "columns" = rep(column, each = 3),
                                     "model" = c("simple",  "mice", "missforest"),
                                     "type" = rep("cat", each = 3))
      
      final_results <- rbind(final_results, accuracy_results)
      
    } else {
      # - - - - For Numerical Columns
      
      if (startsWith(columns[1], "cat") & startsWith(columns[2], "cont")) {
        i = 1 # little fix for when we have one cat and one cont columns to impute as indexing is different
        nrmse <- c(simple_imputer_result[[i + 1]]$nrmse, mice_result$nrmse[[i]], missforest_result$nrmse[[i]])
      } else {nrmse <- c(simple_imputer_result[[i]]$nrmse, mice_result$nrmse[[i]], missforest_result$nrmse[[i]])}
      
      nrmse_results <- data.frame("performance" = nrmse,
                                  "performance_type" = rep("nrmse", each = 3),
                                  "columns" = rep(column, each = 3),
                                  "model" = c("simple", "mice", "forest"),
                                  "type" = rep("cont", each = 3))
      
      final_results <- rbind(final_results, nrmse_results)

    }
    
  }
  
  return(final_results)
}

# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

## FUNCTION
#  Function to extract all mean difference results for statistical testing in one dataframe
## INPUTS
#  simple_imputer_result: results from fitting
#  mice_result: results from fitting
#  missforest_result: results from fitting
#  columns: vector input containing strings representing column names which contain imputed values
## OUTPUT
#  dataframe 
store_mean_diff_results <- function(simple_imputer_result, mice_result, missforest_result, columns) {
  
  # 1. IF scenario {both}
  if (startsWith(columns[1], "cat") & startsWith(columns[2], "cont")) {
    
    mean_diff <- c(simple_imputer_result[[2]]$mean, mice_result$mean[[1]], missforest_result$mean[[1]])
    
    mean_diff_results <- data.frame("mean_diff" = mean_diff,
                                    "model" = c("simple", rep("mice", 3), "forest"))
  
  # 2. IF scenario {cont_only} 
  } else if (startsWith(columns[1], "cont") & startsWith(columns[2], "cont")) {
    
    mean_diff_col_1 <- c(simple_imputer_result[[1]]$mean, mice_result$mean[[1]], missforest_result$mean[[1]])
    mean_diff_col_2 <- c(simple_imputer_result[[2]]$mean, mice_result$mean[[2]], missforest_result$mean[[2]])
    
    mean_diff_results <- data.frame("mean_diff" = c(mean_diff_col_1, mean_diff_col_2),
                                    "model" = rep(c("simple", rep("mice", 3), "forest"), 2))
    
  }
  
  return(mean_diff_results)

}

# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

```

----

#### Note: need to include **data wrangling functions** to transfer data from python to R

``` {r message = FALSE, eval = TRUE, echo = TRUE, warning = FALSE}

# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

## FUNCTION
#  Function wrangles the data in appropriate structure for machine learning imputations.
## INPUTS
#  nans_df: data.frame containing random NA's values
## OUTPUT
#  data frames that are in appropriate state.
wrangle_data_for_na <- function(nans_df) {
  
  # Conversion issue between NaN and NA, hence we can explicitly use
  nans_df[nans_df == 'NaN'] = NA
  
  # Converting the data into R data frames
  nans_df <- as.data.frame(nans_df)
  
  # Unlist all of the columns
  columns <- colnames(nans_df)
  nans_df <- nans_df %>% mutate(across(all_of(columns), unlist))
  
  # All character types must be changed to factors
  nans_df <- nans_df %>% mutate_if(is.character, as.factor)
  
  return(nans_df)
}

# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

```

``` {r message = FALSE, eval = TRUE, echo = FALSE, warning = FALSE}

## FUNCTION
#  Function wrangles the data in appropriate structure for machine learning imputations.
## INPUTS
#  true_df: data.frame containing true values.
## OUTPUT
#  original data frames that are in appropriate state.
wrangle_data_for_original <- function(true_df) {
  
  # Converting the data into R data frames.
  true_df <- as.data.frame(true_df)
  
  # All character types must be changed to factors
  true_df <- true_df %>% mutate_if(is.character, as.factor)
  
  return(true_df)
}

```

``` {r echo = FALSE, eval = TRUE, warning = FALSE}
### FUNCTION
# Its a function that will print the result metrics obtained for each simulation run
# Its used to make code more compact and reproducible. Avoiding duplicate code
## INPUTS
# simulation: String indicating which simulation run the results are
# columns: List holding the value of which columns were used.
# simple: List holding the results involving Simple Imputer
# mice: List holding the results involving Mice Imputer
# forest: List holding the results involving the MissForest Imputer
print_metrics_obtained <- function(simulation, columns, simple, mice, forest) {
  
  cat("For Simulation Run", simulation," the scores are: ", '\n',
    "The mean difference for simple imputer", columns[[1]] ," :", simple[[1]]$mean," ", columns[[2]] ," :", simple[[2]]$mean, '\n',
    "The mean difference for mice ", columns[[1]] ," :", mice$mean$cont0, " ", columns[[2]] ," :", mice$mean$cont1, '\n',
    "The mean difference for missForest ", columns[[1]] ,":", forest$mean[[1]], " ", columns[[2]] ," :", forest$mean[[2]], '\n',
    "The sd difference for simple imputer ", columns[[1]] ,":", simple[[1]]$sd, " ", columns[[2]] ," :", simple[[2]]$sd, '\n',
    "The sd difference for mice ", columns[[1]] ," :", mice$sd$cont0, " ", columns[[2]] ," :", mice$sd$cont1, '\n',
    "The sd difference for missForest ", columns[[1]] ," :", forest$sd[[1]], " ", columns[[2]] ," :", forest$sd[[2]] , '\n')
}

```
***

### 4. Bringing it all together

Need to make a loop over all possible scenario combinations.

#### First, load in data.

``` {r echo = TRUE, message = FALSE}

original_df <- py$read_true_values('data/train.csv')

```

#### Second, set up scenarios.

In order to set up the study, we made a few simplyfing assumptions.

* **Choose two columns to be our reference "predictor columns"** for all combinations of the simulation. This is due to how computationally expensive the fitting of the study is and by extension requires simplifying assumptions. By choosing a set of consistent predictor columns across the study, we control for any variability that could be introduced if these predictor columns changed at every iteration.

  Note, we chose a categorical and a numerical feature for the predictions, this is due to a multicollinearity issue in the prediction of multivariate imputation methods if we had only chosen one type. 

``` {r}

columns_predictions_if_cat <- c('cont0')
columns_predictions_if_cont <- c('cat0')
columns_predictions_if_both <- c('cat0', 'cont0')

```

* **Block out four categorical variables from the study:** namely cat5, cat7, cat8 and cat10. This is because the number of levels of these categorical variables is over 50 and one of the imputation methods cannot handle with this dimensionality. 

  Note, here we are also removing the predictor columns from the possible random picks of columns to be imputed.

``` {r}

cat_vec <- 1:18
cat_vec <- cat_vec[!cat_vec %in% c(5, 7, 8, 10)]

cont_vec <- 1:10

```

* At this stage we did not focus on identifying predictive power of features and fine-tuning the imputation modelling for a specific set of features. Due to the randomness element of the study, we simply **choose the columns to be imputed at random**. The only requirement of the study is that you have the same number of occurrences of columns to be imputed which are $\text{\{only continuous, only categorical, both\}}$.

  This is controlled by looping through each combination and picking out a number of simulation as defined by `num_simulations`.

``` {r echo = TRUE, message = FALSE}

combinations <- c("cont_only", "cat_only", "both")
num_simulations <- 3

i = 0
columns_combinations <- list()
for (combination in combinations) {
  for (iter in 1:num_simulations) {
    i = 1 + i
    
    if (combination == "cont_only") {
      
      cont_num <- sample(cont_vec, 2)
      columns_combinations[[i]] <- c(paste0('cont', cont_num[1]), paste0('cont', cont_num[2]))
      
    } else if (combination == "cat_only") {
      
      cat_num <- sample(cat_vec, 2)
      columns_combinations[[i]] <- c(paste0('cat', cat_num[1]), paste0('cat', cat_num[2]))
      
    } else if (combination == "both") {
      
      cat_num <- sample(cat_vec, 1)
      cont_num <- sample(cont_vec, 1)
      columns_combinations[[i]] <- c(paste0('cat', cat_num), paste0('cont', cont_num))
      
    }
  }
}

```

* **Choose a relevant set of percentages of data that should be dropped**: this is part of the set up of the simulation study and here we are exploring three cases $\text{\{}0.1, 0.2, 0.3\text{\}}$.

``` {r echo = TRUE, eval = FALSE, message = FALSE}

percentage_drop <- list("large" = 0.3, "medium" = 0.2, "small" = 0.1)

```

* **Choose a subset of the data**: this is due to the computational expense of the runs complessively. In order to be able to run more scenarios in a local machine, we chose to take a subset of the data. This tradeoff was necessary due to the lack of access to a desktop computer and being unable to run this code overnight.

``` {r eval = FALSE}

original_df <- original_df[0:100000, ]

```

Then, run the full simulation.

#### Third, full run.

``` {r eval = FALSE, echo = TRUE, message = FALSE, warning = FALSE}

final_results <- mean_diff_results <- c()

# Simulation Loops
i = 0
system.time(for (columns in columns_combinations) {
  for (percentage in percentage_drop) {
    
    i = 1 + i
  
    cat("Starting run", i)
    start_time <- Sys.time()
    cat(".")
    
    ### 1. Drop Values Randomly
    nans_df <- py$remove_random_values(original_df, columns, percentage)
    cat(".")
    
    ### 2. Simple Imputer
    simple_imputer <- py$univariate_imputation_method(nans_df[columns])
    simple_imputer_result <- py$performance_results_py(original_df, simple_imputer, nans_df, columns)
    cat(".", "\n")
    
    mid_time_1 <- Sys.time()
    cat("Simple Imputer Completed - Time Elapsed :", (mid_time_1 - start_time), '\n')
    
    ### 3. Data Wrangling (some Python to R conversions)
    
    # a) Appropriately change features of nans_df to R environment
    nans_df <- wrangle_data_for_na(nans_df)

    # b) Deal with Single Impute Results
    if (startsWith(columns[1], "cat") & startsWith(columns[2], "cat")) {
      names(simple_imputer_result[[1]]) <- names(simple_imputer_result[[2]]) <- c("conf_matrix", "accuracy")
    } else if (startsWith(columns[1], "cont") & startsWith(columns[2], "cont")){
      names(simple_imputer_result[[1]]) <- names(simple_imputer_result[[2]]) <- c("D", "PD", "mean", "sd", "nrmse")
    } else {
      names(simple_imputer_result[[1]]) <- c("conf_matrix", "accuracy")
      names(simple_imputer_result[[2]]) <- c("D", "PD", "mean", "sd", "nrmse")
    }
    names(simple_imputer_result) <- c(columns[1], columns[2])
  
    ### 4. Multivariate Imputers
    
    # a) Set up prediction columns
    
    # IF {cat_only} scenario:
    if (startsWith(columns[1], "cat") & startsWith(columns[2], "cat")) {
      pred_columns <- unique(c(columns, columns_predictions_if_cat))
    
    # IF {cont_only} scenario:
    } else if (startsWith(columns[1], "cont") & startsWith(columns[2], "cont")) {
      pred_columns <- unique(c(columns, columns_predictions_if_cont))
      
    # IF {both} scenario:
    } else { pred_columns <- unique(c(columns, columns_predictions_if_both)) }
    
    # b) MICE Imputer
    mice_imputer <- impute_using_mice(nans_df, pred_columns)
    mice_result <- performance_results_mice(original_df, mice_imputer, columns)
    
    mid_time_2 <- Sys.time()
    cat("MICE Imputer Completed - Time Elapsed :", (mid_time_2 - mid_time_1), '\n')
    
    # c) MissForest Imputer
    missforest_imputer <- invisible(impute_using_missForest(nans_df, pred_columns))
    missforest_result <- performance_results_missForest(original_df, missforest_imputer, nans_df, columns)
    
    mid_time_3 <- Sys.time()
    cat("MissForest Imputer Completed - Time Elapsed :", (mid_time_3 - mid_time_2), '\n')
    
    ### 5. Store Results
    
    iter_results <- store_performance_results(simple_imputer_result, mice_result, missforest_result, columns)
    iter_results <- cbind(iter_results, "% drop" = c(rep(percentage, each = nrow(iter_results))))
    final_results <- rbind(final_results, iter_results)
    
    # IF {cont_only, both} scenarios:
    if (startsWith(columns[1], "cont") & startsWith(columns[2], "cont") | startsWith(columns[1], "cat") & startsWith(columns[2], "cont")) {
      iter_results <- store_mean_diff_results(simple_imputer_result, mice_result, missforest_result, columns)
      iter_results <- cbind(iter_results, "% drop" = c(rep(percentage, each = nrow(iter_results))))
      mean_diff_results <- rbind(mean_diff_results, iter_results)
    }
    
    end_time <- Sys.time()
    cat("Time taken to compute full run", i, "for column combination", columns, "is :", (end_time - start_time), '\n')
    
  }
})

```

Save results!

``` {r eval = FALSE, echo = TRUE, message = FALSE}

saveRDS(final_results, file = "imputation_results.rds")
saveRDS(mean_diff_results, file = "imputation_results_2.rds")

```

---

# Analysing Results of Imputation Methods Performance

---

### Necessary Libraries

``` {r, message = FALSE}

library(ggplot2)
library(dplyr)
library(RColorBrewer)

```

### Read in results

``` {r}

model_performance <- readRDS("imputation_results.rds")
mean_diff_estimates <- readRDS("imputation_results_2.rds")

```

***

## Model Performance

#### Take a look at the model performances output

A snapshot of the results:

Given a total output of 162 observations due to all the possible combinations, a snapshot of this dataframe can be seen below:

``` {r}

head(model_performance, 30)

```

The measures of model performance chosen have been the:

* **Normalised Root Mean Squared Error** for the *Continuous Features*.

  Recall, the formula for the NRMSE is $\sqrt{mean((X_{true} âˆ’ X_{imp})^2)/var(X_{true})}$, where $X_{true}$ the complete data matrix and $X_{imp}$ the imputed data matrix.
  
  In simple words, this metric is made up of the squared error over the deviation of the true underlying data, which normalises the error term and allows for meaningful comparisons across the different column estimates and scenarios. Moreover, for good performance we expect values to be close to zero.

``` {r, echo = TRUE, message = FALSE, fig.height = 6, fig.width = 12}

subset_model_performance <- model_performance[model_performance$type == 'cont', ]

# take group means, where the group of reference is the model
df <- data.frame(subset_model_performance %>% group_by(model) %>% summarize(mean = mean(abs(performance))))

# set up labels names
labels <- c("MissForest Imputer", "MICE Imputer", "Simple Imputer")

# plot
ggplot(subset_model_performance, aes(x = abs(performance), fill = model)) + 
  geom_histogram(bins = 30, alpha = 0.8) +
  geom_vline(data = df, aes(xintercept = mean, colour = model), linetype = "dashed", size = 1) +
  scale_color_manual(values = c(brewer.pal(9, "YlGnBu"))[c(4, 6, 8)], labels = labels) + 
  scale_fill_manual(values = c(brewer.pal(9, "YlGnBu"))[c(4, 6, 8)], labels = labels) + 
  labs(title = "Model Comparison of NRMSE across all Scenarios for Numerical Features",
       subtitle = "Smaller NRMSE implies better performance", x = "NRMSE", y = "Count")

ggsave("nrmse_comparison.png", plot = last_plot())

```

Snapshot of the first 20 *results, sorted from best to worst,* which confirm what observed in the plot:

``` {r eval = TRUE, echo = FALSE, message = FALSE}

subset <- model_performance[model_performance$performance_type == 'nrmse', ]
head(subset[order(subset$performance, decreasing = FALSE), c("performance", "performance_type", "model", "type", "% drop")], 20)

```

The results clearly show that on average, as highlighted by the vertical lines, the **missforest imputation modelling performs best, followed by the simple imputer and the mice imputer** in this order. It appears that MissForest, even though it was computationally the most expensive, is a good choice for imputing continuous features.

Moreover, the results in general seem *uneffected by the % of values dropped*.

Due to the multivariate nature of the imputation methods (missForest and MICE) it is apparent that the key driver behind the performance is how explanatory the predictor columns are for the randomly chosen columns to impute. Due to the key difference in assumptions between missForest and MICE, namely *non-parametric vs parametric* approach, it seems that the continuous features are better explained by non-linear relationships with the explanatory columns. Even though finding the explanatory columns was not of interest in this simulation study, it is still an interesting realisation of the results. The predictive power of the features will be explored in the later part of the project. 

* **Accuracy** for the *Categorical Features*

  Recall accuracy is the sum the counts in the *diagonal* of the confusion matrix (True Positive and True Negative) and divide by the sum of all entries in the confusion matrix.

  In simple words, it captures how many imputed values were correctly identified over the total number of values which were imputed. Moreover, good performance leads to a value close to 1 and bad performance to a value around 0.

``` {r, echo = FALSE, message = FALSE, fig.height = 6, fig.width = 12}

subset_model_performance <- model_performance[model_performance$type == 'cat', ]

# take group means, where the group of reference is the model
df <- data.frame(subset_model_performance %>% group_by(model) %>% summarize(mean = mean(abs(performance))))

# set up labels names
labels <- c("MICE Imputer", "Simple Imputer", "MissForest Imputer")

# plot
ggplot(subset_model_performance, aes(x = abs(performance), fill = model)) + 
  geom_histogram(bins = 30, alpha = 0.8) +
  geom_vline(data = df, aes(xintercept = mean, colour = model), linetype = "dashed", size = 1) +
  scale_color_manual(values = c(brewer.pal(9, "YlGnBu"))[c(6, 8, 4)], labels = labels) + 
  scale_fill_manual(values = c(brewer.pal(9, "YlGnBu"))[c(6, 8, 4)], labels = labels) + 
  labs(title = "Model Comparison of Accuracy across all Scenarios for Categorical Features",
       subtitle = "Larger accuracy implies better performance", x = "Accuracy", y = "Count")

ggsave("accuracy_comparison.png", plot = last_plot())
  
```

Snapshot of the first 20 *results, sorted from best to worst,* which confirm what observed in the plot:

``` {r eval = TRUE, echo = FALSE, message = FALSE}

subset <- model_performance[model_performance$performance_type == 'accuracy', ]
head(subset[order(subset$performance, decreasing = TRUE), c("performance", "performance_type", "model", "type", "% drop")], 20)

```

For accuracy results we observe a different outcome, with the **univariate imputer performing best, mice following in second place and missforest placing last**. This is likely due to the fundamental assumptions of the multivariate methods not being met by the data. The chosen predictor continuous covariate $\text{cont0}$ is likely not indicative of the behaviour of the randomly chosen categorical columns to impute.

This is a clear case of the strength of the simple imputer on average, given that we have a very complex structure of the data and finding the appropriate predictor columns can often be a challenge. In these instances where the relationship of the data is not immediately clear, it could be often recommended to rely on a univariate imputation method. In both scenarios, continuous and discrete, the simple imputer performs well overall.  

***

## Mean Difference Estimates

#### Take a look at the estimated output

A snapshot of this dataframe can be seen below

``` {r}

head(mean_diff_estimates, 20)

```

##### What do I mean by mean difference?

A good way to assess model performance in this instance, is to take a look at the **ability of each model to correctly impute the missing values**. This measure is obtained by taking the **average of the differences** between all the imputed values $\bar{x}_{\text{imp}, i}$ for column $i$ and all the true values $\bar{x}_{\text{true}, i}$ for column $i$.

Hence: $MD_i = mean(\bar{x}_{\text{imp}, i} - \bar{x}_{\text{true}, i})$

This measure is computed for each column of interest, where the column i was randomly chosen in the simulation as the column to drop values from, and for each scenario combination, where the scenario combinations are {cont_only, 0.1}, {cat_only, 0.1}, {both, 0.1}, {cont_only, 0.2}, ... .

##### How do I make use of these measures?

First, one may wish to take a look at a group comparison. This can help highlight better performing models against poorer performing models in one go.

* For *poor performance* of a model, I expect to see the distribution of $MD$ with an overall **mean further away from zero and a high variability**.
* For *good performance* of a model, I expect to see the distribution of $MD$ with an overall **mean closer to zero and a smaller variability**. 

Therefore;

``` {r, echo = FALSE, message = FALSE, fig.height = 6, fig.width = 14}

# take group means, where the group of reference is the model
df <- data.frame(mean_diff_estimates %>% group_by(model) %>% summarize(mean = mean(mean_diff)))

# set up labels names
labels <- c("MissForest Imputer", "MICE Imputer", "Simple Imputer")

# plot
ggplot(mean_diff_estimates, aes(x = mean_diff, fill = model)) + 
  geom_histogram(bins = 30, alpha = 0.8) +
  geom_vline(data = df, aes(xintercept = mean, colour = model), linetype = "dashed", size = 1) +
  scale_color_manual(values = c(brewer.pal(9, "YlGnBu"))[c(4, 6, 8)], labels = labels) + 
  scale_fill_manual(values = c(brewer.pal(9, "YlGnBu"))[c(4, 6, 8)], labels = labels) + 
  labs(title = "Model Comparison of Absolute Mean Differences across all Scenarios", x = "Subpopulation Mean Difference", y = "Count")

ggsave("md_comparison.png", plot = last_plot())
  
```

This plot shows a similar information as the one represented by the NRMSE previously. Here, we have the distribution of the mean differences about zero, and it appears the models perform well with the missforest and the simple imputer head-to-head and the MICE performing worse than them. 
This confirms the conclusions drawn from the NRMSE metric

***

### Limitations and Areas for Development

* **In terms of the limitations of the study and scenarios**:

  This simulation study captures *only a snapshot of the potential analysis over the imputation methods*. This is due to the simplifying assumptions that had to be made, because of the lack of more powerful equipment and the limited amount of time we had available to work on it.

  It is important to note that the framework and the functions built in this study could support much more complex analysis and scenario building. However this could not be explored further for this report, for the same reasons mentioned above.
  
* **In terms of the models chosen**:

  We strongly believe that the models chosen cover a wide range of fundamental assumptions and are therefore suited for this analysis. This enabled us to draw some inference and evaluate likely causes for the poor performance of certain methods such as MICE. However, a more comprehensive way could have been to test the model assumptions first.
  
  Moreover, the 'prediction columns' were chosen arbitrarily and *another set of prediction columns could have led to a different outcome*. For this reason a more comprehensive analysis could have been to allow for the training of the multivariate methods on the entire dataset. If it wasn't for the computational limitations, this route could have been preferred.

* **In terms of the performance analysis**:

  We would have also liked to explore further the test statistic of the Difference, and carry out some statistical testing to improve the confidence of our results. One of the possible tests we considered was a **paired t-test** where the pair would be $(X_{\text{imp}}, X_{\text{true})}$, and the null hypothesis would be $H_0 : \mu_{X_{\text{imp}}} = \mu_{X_{\text{true}}})$.
  
  This test could have been repeated for each imputed column and could have highlighted the instances for which the imputed values diverged significantly from the true values, to the extent of having to reject the null hypothesis that the difference of the means is zero.
  
***

``` {python echo = FALSE, eval = FALSE, message = FALSE}

##### a) READING DATA AND UNIVARIATE IMPUTATION (PYTHON)

### READING AND REPLACING DATA WITH NA'S ##

# Reading in data frame.
original_df = read_true_values('data/train.csv')

# Simulated scenarios of removal data.
# [1] two continuous variables (cont0, cont1) with 10% removal
# [2] two categorical variables (cat0, cat1) with 10% removal
# [3] one continuous and one categorical (cont0, cat0) with 10% removal

# Column holding values.
column_one = ['cont0', 'cont1']
column_two = ['cat0', 'cat1']
column_three = ['cont0', 'cat0']

# Removing...
# YOU MUST USE .copy() SO IT IS NOT REFERENCE BASED!!!
nans_one = remove_random_values(original_df.copy(), column_one, 0.1 )
#nans_two = remove_random_values(original_df.copy(), column_two, 0.1 )
#nans_three = remove_random_values(original_df.copy(), column_three, 0.1 )

# UNIVARIATE IMPUTATION TECHNIQUE is in python so we apply now.
# Predicted values
simple_imputer_one = univariate_imputation_method(nans_one[column_one])
#simple_imputer_two = univariate_imputation_method(nans_two[column_two])
#simple_imputer_three = univariate_imputation_method(nans_three[column_three])

# COMPUTE UNIVARIATE PERFORMANCE METRICS
simple_imputer_one_results = performance_results_py(original_df, simple_imputer_one, nans_one, column_one)

# This is necessary due to bug in transformation of dataset from Python to R 
nans_one = nans_one.replace(np.nan, 'NA')
#nans_two = nans_two.replace(np.nan, 'NA')
#nans_three = nans_three.replace(np.nan, 'NA')

```

``` {r echo = FALSE, eval = FALSE, message = FALSE, warning = FALSE}

# Need to bring original and random removal data in the R enviroment.

# Original Data.
original_df <- py$original_df

# Creating the column vectors being used.
column_one = c('cont0', 'cont1', 'cat0')
column_two = c('cat0', 'cat1', 'cont1')
column_three = c('cont0', 'cat0', 'cont3')

# NA data of the different simulation scenarios.
nans_one <- py$nans_one
#nans_two <- py$nans_two
#nans_three <- py$nans_three

# Wrangle Data in Appropriate Data.
nans_one <- wrangle_data_for_na(nans_one)
#nans_two <- wrangle_data_for_na(nans_two)
#nans_three <- wrangle_data_for_na(nans_three)

original_df <- wrangle_data_for_original(original_df)

```

``` {r echo = FALSE, eval = FALSE, message = FALSE, warning = FALSE}

# Transferring Simple Imputer Results to R

simple_imputer_one_results <- py$simple_imputer_one_results
names(simple_imputer_one_results[[1]]) <- c("D", "PD", "mean", "sd")
names(simple_imputer_one_results[[2]]) <- c("D", "PD", "mean", "sd")
names(simple_imputer_one_results) <- c(py$column_one[1], py$column_one[2])

```

``` {r echo = FALSE, eval = FALSE, message = FALSE, warning = FALSE}

##### b) MICE IMPUTATION (R)

# Imputing data using MICE.

mice_imputer_one <- impute_using_mice(nans_one, column_one)
# mice_imputer_two <- impute_using_mice(nans_two, column_two)
# mice_imputer_three <- impute_using_mice(nans_three, column_three)

```

``` {r echo = FALSE, eval = FALSE}

# Need to remove non imputed columns. 
column_one = c('cont0', 'cont1')
column_two = c('cat0', 'cat1')
column_three = c('cont0', 'cat0')

```

``` {r echo = FALSE, eval = FALSE}

# Obtaining performance metrics for the MICE imputed data.

mice_result_one <- performance_results_mice(original_df, mice_imputer_one, column_one)
# mice_result_two <- performance_results_mice(original_df, mice_imputer_two, column_two)
# mice_result_three <- performance_results_mice(original_df, mice_imputer_three, column_three)

```

``` {r echo = FALSE, eval = FALSE}

##### c) MISSFOREST IMPUTATION (R)

# Creating the column vectors being used.
column_one = c('cont0', 'cont1', 'cat0')
column_two = c('cat0', 'cat1', 'cont1')
column_three = c('cont0', 'cat0', 'cont3')

```

``` {r echo = FALSE, eval = FALSE}

# Imputing Data using missForest.

missForest_imputer_one <- impute_using_missForest(nans_one, column_one)
# missForest_imputer_two <- impute_using_missForest(nans_two, column_two)
# missForest_imputer_three <- impute_using_missForest(nans_three, column_three)

```

``` {r echo = FALSE, eval = FALSE}

# Need to remove non imputed columns. 
column_one = c('cont0', 'cont1')
column_two = c('cat0', 'cat1')
column_three = c('cont0', 'cat0')

```

``` {r echo = FALSE, eval = FALSE}

# Obtaining performance metrics for the missforest imputed data.

missForest_result_one <- performance_results_missForest(original_df, missForest_imputer_one, nans_one, column_one)
# missForest_result_two <- performance_results_missForest(original_df, missForest_imputer_two, nans_two, column_two)
# missForest_result_three <- performance_results_missForest(original_df, missForest_imputer_three, nans_three, column_three)

```

``` {r echo = FALSE, eval = FALSE, warning = FALSE}

##### c) FINAL SCORES

# The final scores of the mean difference between true and predicted, as well as their respective standard deviations are:

# Compare results final.
print_metrics_obtained("One", column_one, simple_imputer_one_results, mice_result_one, missForest_result_one)

```

``` {r  eval = FALSE, echo = FALSE, warning = FALSE}
##### PLOTS

### FUNCTION
## Function to plot density plots to comparing numeric column imputations
## Input
# simulation: String holding the number of simulation and covariate exploring
# simple: List of all the D values for simple imputation
# micee: List of all D values for mice imputation
# forest: List of all D values for missForest imputations
plot_density <- function(simulation, Simple, Mice, MissForest) {
 
  # Must be <<data.frame>> not <<as.data.frame>> . Otherwise it does not create it correctly for some reason..
  plotingData <- data.frame(Simple,
                            Mice,
                            MissForest)
  
  # Convert data to fit densities
  # Imputation: column that will hold the old column names
  # Difference: column that will hold all the Differences computed for all imputers
  # Line under, columns we are gathering.
  densPlotting <- plotingData %>%
    gather(Imputation, Difference,
           Mice, MissForest, Simple)
  
  # Giving appropriate title for graph.
  title <- paste("Density Plot comparing techniques for Simulation", simulation)
  
  # Plotting the data using ggplot.
  ggplot(densPlotting, aes(Difference, fill = Imputation, colour = Imputation)) + 
    geom_density(alpha = 0.1) +
    xlab("Difference of True-Imputed value in Smilatuon") + 
    ylab("Density") + 
    ggtitle(title) 
}

# Calling the funciton to plot the data.
plot_density("One 'cont0'",
             c(simple_imputer_one_results$cont0$D),
             c(mice_result_one$D$cont0[[1]]),
             c(missForest_result_one$D$cont0))

```

``` {r  eval = FALSE, echo = FALSE, warning = FALSE}
### FUNCTION
## Function to plot error plots to comparing numeric column imputations
## Input
# simulation: String holding the number of simulation and covariate exploring
# means: List holding mean values for each imputation (simple, missForest, mice)
# sds: List holding sd values for each imputation (simple, missForest, mice)
# imputations: List holding the name imputations methods (simple, missForest, mice)
plot_error <- function(simulation, means, sds, imputations) {
  
  # Creating the data frame for the plot.
  error_data <- data.frame(means,
                           sds,
                           imputations)
  
  # Creating appropriate title for graph
  title <- paste("Comparing CI of mean difference produces for Simulation", simulation)
  
  # Plotting the data
  ggplot(error_data, aes(x = imputations, color = imputations)) +
  geom_errorbar(aes(ymax = (means + sds), 
                    ymin =(means - sds)), position = "dodge") +
  xlab("Imputation Method") + 
  ylab("Mean +/- 1SD of the raw difference") +
  ggtitle(title)

  
}

# Calling function to plot error bars (compare mean's and sd's)
plot_error("One 'cont0'",
           c(simple_imputer_one_results$cont0$mean, missForest_result_one$mean$cont0, mice_result_one$mean$cont0[[1]]),
           c(simple_imputer_one_results$cont0$sd, missForest_result_one$sd$cont0, mice_result_one$sd$cont0[[1]]),
           c("Simple", "MissForest", "Mice"))

```


``` {r  eval = FALSE, echo = FALSE, warning = FALSE}
actual_plot <- function(title, dataPlot) {
    # Giving appropriate title for graph.
  title <- paste(title)
  # Plotting the data using ggplot.
  ggplot(dataPlot, aes(Difference, fill = Imputation, colour = Imputation)) + 
    geom_density(alpha = 0.1) +
    geom_segment(aes(x=(mean(Difference) + 1.96*sd(Difference)),
                     xend=(mean(Difference) + 1.96*sd(Difference)),
                     y=0, yend=3), alpha=0.5, lty="21") +
    geom_segment(aes(x=(mean(Difference) - 1.96*sd(Difference)),
                     xend=(mean(Difference) - 1.96*sd(Difference)),
                     y=0, yend=3), alpha=0.5, lty="21") +
    xlab("Difference of True-Imputed value in Smilatuon") + 
    ylab("Density") + 
    ggtitle(title) 
}

MissForest <-c(missForest_result_one$D$cont0)
plotingData <- data.frame(MissForest)
  # Convert data to fit densities
  # Imputation: column that will hold the old column names
  # Difference: column that will hold all the Differences computed for all imputers
  # Line under, columns we are gathering.
plotingData <- plotingData %>%
  gather(Imputation, Difference,
          MissForest)
actual_plot(paste("MissForest Imputation CI with density for Simulation One"), plotingData)

Mice <- c(mice_result_one$D$cont0[[1]])
plotingData <- data.frame(Mice)
plotingData <- plotingData %>%
  gather(Imputation, Difference,
         Mice)
actual_plot(paste("Mice Imputation CI with density for Simulation One"), plotingData)
  
Simple <- c(simple_imputer_one_results$cont0$D)
plotingData <- data.frame(Simple)
plotingData <- plotingData %>%
  gather(Imputation, Difference,
         Simple)
actual_plot(paste("Simple Imputation CI with density for Simulation One"), plotingData)

rm(list = c("Simple", "Mice", "MissForest", "plotingData"))

```

``` {r  eval = FALSE, echo = FALSE, warning = FALSE}
####################
# WILL BE INPUTTED IN FUNCTION
###################
### FUNCTION
## Function to plot density plots to comparing numeric column imputations
## Input
# simulation: String holding the number of simulation and covariate exploring
# simple: List of all the D values for simple imputation
# mice: List of all D values for mice imputation
# forest: List of all D values for missForest imputations
plot_density_histogram <- function(simulation, Simple, Mice, MissForest) {
 
  # Have to find lower and upper bounds for each metric.
  lower_simple <- mean(Simple) - 1.96 * sd(Simple)
  upper_simple <- mean(Simple) + 1.96 * sd(Simple)
  
  lower_mice <- mean(Mice) - 1.96 * sd(Mice)
  upper_mice <- mean(Mice) + 1.96 * sd(Mice)
  
  lower_forest <- mean(MissForest) - 1.96 * sd(MissForest)
  upper_forest <- mean(MissForest) + 1.96 * sd(MissForest)
  
  
  # Keeping Differences contained in the CI of each imputation.
  forest_CI <- MissForest[MissForest > lower_forest]
  forest_CI <- MissForest[MissForest < upper_forest]
  
  mice_CI <- Mice[Mice > lower_mice]
  mice_CI <- Mice[Mice < upper_mice]
  
  simple_CI <- Simple[Simple > lower_simple]
  simple_CI <- Simple[Simple < upper_simple]
  
  N_observ <- c(length(simple_CI),
                length(mice_CI),
                length(forest_CI))
  
  # Finding proportion that fall in CI
  percentage <- c(length(simple_CI) / length(Simple) * 100,
                length(mice_CI) / length(Mice) * 100,
                length(forest_CI) / length(MissForest) * 100)
  # Putting values in data frame to compare
  Imputation <- c("Simple", "Mice", "MissForest")
  compareData <- data.frame(N_observ,
                            Imputation,
                            percentage)
  
  # Giving appropriate title for graph.
  title <- paste("Number of Observation obtained in 95% CI", simulation)
  
  
  # Plotting Data in Histogram
  ggplot(compareData, aes(x = Imputation, y = N_observ)) +
    geom_bar(stat = "identity", aes(fill = Imputation, colour = Imputation)) +
    geom_text(label = paste(round(percentage, digits=2), "%"),
              hjust = 1.5, size = 5, position = position_dodge(width = .75)) +
    coord_flip() + 
    xlab("Imputation Technique") + 
    ylab("Number of Observations in 95% CI") + 
    ggtitle(title) 
  
  
}



# Calling the funciton to plot the data.
plot_density_histogram("One 'cont0'",
             c(simple_imputer_one_results$cont0$D),
             c(mice_result_one$D$cont0[[1]]),
             c(missForest_result_one$D$cont0))


```



``` {r  eval = FALSE, echo = FALSE, warning = FALSE}
####################
# TESTER TO FIX ERRORS IN ONE PLOT.
###################

plot_density_with_CI <- function(simulation, Simple, Mice, MissForest) {
 
  # Must be <<data.frame>> not <<as.data.frame>> . Otherwise it does not create it correctly for some reason..
  plotingData <- data.frame(Simple,
                            Mice,
                            MissForest)
  
  # Convert data to fit densities
  # Imputation: column that will hold the old column names
  # Difference: column that will hold all the Differences computed for all imputers
  # Line under, columns we are gathering.
  densPlotting <- plotingData %>%
    gather(Imputation, Difference,
           Simple, Mice, MissForest)
  
  
  # Giving appropriate title for graph.
  title <- paste("Density Plot comparing techniques for Simulation", simulation)

  # Plotting the data using ggplot.
  ggplot(densPlotting, aes(Difference, fill = Imputation, colour = Imputation)) + 
    geom_density(alpha = 0.1) +
    geom_segment(aes(x=mean(Difference) + 1.96*sd(Difference), xend=mean(Difference) + 1.96*sd(Difference),
                     y=0, yend=3), alpha=0.5, lty="21") +
    geom_segment(aes(x=mean(Difference) - 1.96*sd(Difference), xend=mean(Difference) - 1.96*sd(Difference),
                     y=0, yend=3), alpha=0.5, lty="21") +
   #  geom_text(data = compareData, aes( y = 2, label = paste(N_observ)), check_overlap = TRUE) +
    xlab("Difference of True-Imputed value in Smilatuon") + 
    ylab("Density") + 
    ggtitle(title) +
    facet_wrap(~Imputation)
}



# Calling the funciton to plot the data.
plot_density_with_CI("One 'cont0'",
             c(simple_imputer_one_results$cont0$D),
             c(mice_result_one$D$cont0[[1]]),
             c(missForest_result_one$D$cont0))


```

***

``` {python echo = FALSE, eval = FALSE}

def imputation_complete(complete_df, columns, percentage):
    """
    Function which brings together all the functions for a single step of the simulation study.
    These consists of:
    1. randomly dropping values from the dataset,
    2. fitting imputation methods, and
    3. evaluating the performance of the imputed data against the true data.

    :param df: pandas DataFrame which is the full data
    :param columns: list of columns to 'drop' values from. Note: must be string name of columns
    :param percentage: float input such as 0.20 (i.e. 20%)
    :return: results for each of the methods (currently only missforest and median imputer)
    """

    ##### COMPLETE CODE:

    ### 1. DROP VALUES RANDOMLY

    df_nans = remove_random_values(complete_df.copy(), columns, percentage)

    ### 2. COMPUTE METHODS

    median_imputed_data, missforest_imputed_data = univariate_imputation_methods(df_nans[columns])

    ### 3. EXTRACT TRUE VALUES AND COMPARE WITH IMPUTED VALUES

    results_median = []
    results_missforest = []
    for column in columns:

        # a. Extract boolean vector (True/False) to identify Imputed True Data Vectors
        boolean_values = (original_df != df_nans)[column].to_numpy().tolist()
        true_values = original_df[column][boolean_values]

        # c. Extract Imputed Data Vectors
        imputed_values_median = median_imputed_data[column][boolean_values].to_numpy().tolist()
        imputed_values_missforest = missforest_imputed_data[column][boolean_values].to_numpy().tolist()

        # d. Evaluate performance metrics
        results_median.append(performance_metrics(imputed_values_median, true_values))
        results_missforest.append(performance_metrics(imputed_values_missforest, true_values))

    return(results_median, results_missforest)


```