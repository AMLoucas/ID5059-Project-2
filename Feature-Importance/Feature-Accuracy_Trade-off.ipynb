{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "comparative-chase",
   "metadata": {},
   "source": [
    "# Feature Importance in detail\n",
    "\n",
    "## Are all features needed?\n",
    "\n",
    "This notebook will examine in depth if all the data being collected is needed to predict the clients target. We will be using a XGBoost classifier to predict the the target and obtain our accuracy scores from.\n",
    "\n",
    "### Workflow of the process\n",
    "\n",
    " 1. Import data and libraries\n",
    " 2. Wrangle data and compute feature importance\n",
    " 3. Re-construct beginning state of data with 'covariate importance'\n",
    " 4. Fit XGBoost Classifier with different number of covariates\n",
    "  * With 5 most feature important\n",
    "  * With 10 most feature important\n",
    "  * With 15 most feature important\n",
    "  * With 20 most feature important\n",
    "  * With 25 most feature important\n",
    "  * With 30 most feature important\n",
    " 5. Plot the Acuuracy with Number of covariates trade-off\n",
    " \n",
    "To run the notebook, make sure you all the appropriate software and libraries downloaded on your local machine.\n",
    "\n",
    "# Import libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "velvet-injury",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important libraries for data frame manipulations.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys as sys\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# For fitting a XGBClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# To create graphs/plots to visualisation feature relations.\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "affecting-plastic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the training and testing set already provided.\n",
    "training_data = pd.read_csv(\"data/train.csv\")\n",
    "testing_data = pd.read_csv(\"data/test.csv\")\n",
    "submit_file = pd.read_csv(\"data/sample_submission.csv\", index_col='id')\n",
    "\n",
    "# Isolating the response value.\n",
    "trainY_response = training_data['target'].copy()\n",
    "\n",
    "# Dropping the values not needed in the datasets\n",
    "# From training = [targer, id]\n",
    "# From tesitng = [id]\n",
    "training_data.drop('target', axis = 1, inplace = True)\n",
    "testing_data.drop('id', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funded-parks",
   "metadata": {},
   "source": [
    "We need to One-Hote-Encode the training and testing set with the same structure to find the feature importance of each feature in the dataset.\n",
    "\n",
    "### [2] Wrangle Data and compute importance value of covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "referenced-business",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FUNCTION that one hot encodes the 2 datasets together.\n",
    "## Merged dataset will be send and it will be returned 1-Hot-Encoded\n",
    "def find_one_encoder(train, test):\n",
    "    ## MERGED DATA\n",
    "    # This will concatenate the test data rows below the train data\n",
    "    # The first 300K rows will be the train.\n",
    "    # The last 200K rows will be the testing.\n",
    "    merged_encoder = pd.concat([train, test])\n",
    "\n",
    "\n",
    "    # Using get_dummies() which is the same thing as 1-hot-encoder but ignores numerical values.\n",
    "    merged_encoder = pd.get_dummies(merged_encoder)\n",
    "    return(merged_encoder)\n",
    "\n",
    "\n",
    "# First 300K rows is our training set.\n",
    "full_train_X_data = find_one_encoder(training_data.copy(),testing_data.copy()).iloc[:300000,:]\n",
    "# Last 200K rows is our testing set.\n",
    "full_test_X_data = find_one_encoder(training_data.copy(),testing_data.copy()).iloc[300000:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "appointed-bahamas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 300000 entries, 0 to 299999\n",
      "Columns: 643 entries, id to cat18_D\n",
      "dtypes: float64(12), uint8(631)\n",
      "memory usage: 210.3 MB\n"
     ]
    }
   ],
   "source": [
    "# Checking if the computation was done correctly.\n",
    "full_train_X_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "generous-charter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 200000 entries, 0 to 199999\n",
      "Columns: 643 entries, id to cat18_D\n",
      "dtypes: float64(12), uint8(631)\n",
      "memory usage: 140.2 MB\n"
     ]
    }
   ],
   "source": [
    "full_test_X_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artificial-vitamin",
   "metadata": {},
   "source": [
    "Feature importance will be found using a decision tree. The covariates used to split the data closest to the root have the highest info gain. This means there are more important than others in predicting our target goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "moral-projection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 35 s, sys: 1.49 s, total: 36.5 s\n",
      "Wall time: 37.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', random_state=5059)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Fitting a random forest with only 5 trees to get feature importance.\n",
    "DT_feature_importance = DecisionTreeClassifier(criterion='entropy', random_state=5059)\n",
    "DT_feature_importance.fit(full_train_X_data, trainY_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "artificial-lobby",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Feature</th>\n",
       "      <th>Feature Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>632</td>\n",
       "      <td>cat16_B</td>\n",
       "      <td>0.222815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>cont5</td>\n",
       "      <td>0.064849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>cont1</td>\n",
       "      <td>0.048936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>cont6</td>\n",
       "      <td>0.047512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>cont4</td>\n",
       "      <td>0.043646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638</th>\n",
       "      <td>383</td>\n",
       "      <td>cat10_DH</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>382</td>\n",
       "      <td>cat10_DG</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640</th>\n",
       "      <td>380</td>\n",
       "      <td>cat10_DE</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>379</td>\n",
       "      <td>cat10_DD</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>359</td>\n",
       "      <td>cat10_CH</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>643 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index   Feature  Feature Score\n",
       "0      632   cat16_B       0.222815\n",
       "1        6     cont5       0.064849\n",
       "2        2     cont1       0.048936\n",
       "3        7     cont6       0.047512\n",
       "4        5     cont4       0.043646\n",
       "..     ...       ...            ...\n",
       "638    383  cat10_DH       0.000000\n",
       "639    382  cat10_DG       0.000000\n",
       "640    380  cat10_DE       0.000000\n",
       "641    379  cat10_DD       0.000000\n",
       "642    359  cat10_CH       0.000000\n",
       "\n",
       "[643 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding the features that are important.\n",
    "features = []\n",
    "feature_score = []\n",
    "# Appenind all scores in arrays to create a data frame.\n",
    "for i, column in enumerate(full_train_X_data):\n",
    "    features.append(column)\n",
    "    feature_score.append(DT_feature_importance.feature_importances_[i])\n",
    "    \n",
    "# Create a dataframe with these arrays.\n",
    "feature_score_df = zip(features, feature_score)\n",
    "feature_score_df = pd.DataFrame(feature_score_df, columns = ['Feature', 'Feature Score'])\n",
    "\n",
    "# Sort the data frame according to feature score.\n",
    "feature_score_df = feature_score_df.sort_values('Feature Score', ascending=False).reset_index()\n",
    "feature_score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-validation",
   "metadata": {},
   "source": [
    "### [3] Re-construct and find covariate importance\n",
    "\n",
    "We will find the whole column(covariate) importance of the starting by summing all levels feature importance. We will then subset the datasts, in terms of whole most important covariates for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "verbal-anthropology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('cat16', 0.22486957172363686)\n",
      "('cont5', 0.0648488148141639)\n",
      "('cont1', 0.04893635600156071)\n",
      "('cont6', 0.04751204536228715)\n",
      "('cont4', 0.04364580514968128)\n",
      "('cont2', 0.042974862608814786)\n",
      "('cont8', 0.042876331017358466)\n",
      "('cont3', 0.04280314254448044)\n",
      "('id', 0.041575329457989973)\n",
      "('cont9', 0.04005312493535506)\n",
      "('cont0', 0.039723254284598074)\n",
      "('cont10', 0.039454475015376037)\n",
      "('cont7', 0.03807500891209623)\n",
      "('cat7', 0.02662767003316534)\n",
      "('cat10', 0.02529424803206638)\n",
      "('cat8', 0.02382813025380267)\n",
      "('cat1', 0.0221210901136999)\n",
      "('cat18', 0.02107364327017766)\n",
      "('cat0', 0.016921551114809995)\n",
      "('cat2', 0.015969833618716168)\n",
      "('cat15', 0.015574059413575325)\n",
      "('cat14', 0.011888233523265191)\n",
      "('cat4', 0.011651864598939141)\n",
      "('cat6', 0.009872855879651707)\n",
      "('cat11', 0.00881436395636135)\n",
      "('cat9', 0.008797914696591885)\n",
      "('cat3', 0.008405162821917571)\n",
      "('cat17', 0.0067234093454962285)\n",
      "('cat5', 0.0043530158132720565)\n",
      "('cat12', 0.0024455332120263858)\n",
      "('cat13', 0.0022892984750661734)\n"
     ]
    }
   ],
   "source": [
    "# Creating a dictionary to find sum of feature importance scores for each full covariate\n",
    "covariate_importance = {}\n",
    "# Loop through all the features\n",
    "for index, row in feature_score_df.iterrows():\n",
    "    # Capture only covariate name\n",
    "    col = row['Feature'].split('_')[0]\n",
    "    # Capture feature importance score\n",
    "    score = row['Feature Score']\n",
    "    # Add the feature importance score with the name in dictionary and append.\n",
    "    if col in covariate_importance:\n",
    "        covariate_importance[col] += score\n",
    "    else:\n",
    "        covariate_importance[col] = score\n",
    "\n",
    "# Sort the dictionary according to the dictionary values.\n",
    "sorted_by_value = sorted(covariate_importance.items(),key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the dictionary in a presentable way.\n",
    "for covariate in sorted_by_value:\n",
    "    print(covariate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ranking-period",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the top most important covariates in arrays.\n",
    "top5 = ['cat16','cont5', 'cont1', 'cont6', 'cont4']\n",
    "top10 = ['cat16','cont5', 'cont1', 'cont6', 'cont4', 'cont2', 'cont3','cont8', 'cont9', 'cont10']\n",
    "top15 = ['cat16','cont5', 'cont1', 'cont6', 'cont4', 'cont2', 'cont3','cont8', 'cont9', 'cont10', 'cont0', 'cont7',\n",
    "        'cat7','cat10', 'cat8' ] \n",
    "top20 = ['cat16','cont5', 'cont1', 'cont6', 'cont4', 'cont2', 'cont3','cont8', 'cont9', 'cont10', 'cont0', 'cont7',\n",
    "        'cat7','cat10', 'cat8', 'cat1','cat18','cat2','cat0','cat15'] \n",
    "top25 = ['cat16','cont5', 'cont1', 'cont6', 'cont4', 'cont2', 'cont3','cont8', 'cont9', 'cont10', 'cont0', 'cont7',\n",
    "        'cat7','cat10', 'cat8', 'cat1','cat18','cat2','cat0','cat15','cat14','cat4', 'cat6','cat9', 'cat3']\n",
    "\n",
    "# Subseting data to top5 cov\n",
    "training_5 = training_data[top5].copy()\n",
    "testing_5 = testing_data[top5].copy()\n",
    "\n",
    "# Subseting data to top10 cov\n",
    "training_10 = training_data[top10].copy()\n",
    "testing_10 = testing_data[top10].copy()\n",
    "\n",
    "# Subseting data to top15 cov\n",
    "training_15 = training_data[top15].copy()\n",
    "testing_15 = testing_data[top15].copy()\n",
    "\n",
    "# Subseting data to top20 cov\n",
    "training_20 = training_data[top20].copy()\n",
    "testing_20 = testing_data[top20].copy()\n",
    "\n",
    "# Subseting to top25 cov\n",
    "training_25 = training_data[top25].copy()\n",
    "testing_25 = testing_data[top25].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unsigned-impact",
   "metadata": {},
   "source": [
    "### [4] Start trainign XGBoost with different datasets\n",
    "\n",
    "### For top 5 Covariates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "technological-turner",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/jupyterlab/3.0.9/libexec/lib/python3.9/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:10:20] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "CPU times: user 1min 5s, sys: 126 ms, total: 1min 5s\n",
      "Wall time: 16.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=42,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# First 300K rows is our training set.\n",
    "top5_train_X_data = find_one_encoder(training_5,testing_5).iloc[:300000,:]\n",
    "# Last 200K rows is our testing set.\n",
    "top5_test_X_data = find_one_encoder(training_5,testing_5).iloc[300000:,:]\n",
    " \n",
    "# Constructing the XGBClassifier \n",
    "xgb_model = XGBClassifier(random_state = 42)\n",
    "# Fitting the XGBClassifier on full dataset.\n",
    "xgb_model.fit(top5_train_X_data, trainY_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "chief-amber",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/jupyterlab/3.0.9/libexec/lib/python3.9/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Predicting values using the test X matrix\n",
    "y_predicted = xgb_model.predict_proba(top5_test_X_data)[:,1]\n",
    "# Calling function to write the csv file\n",
    "submit_file['target'] = y_predicted\n",
    "submit_file.to_csv('top5-xgboost.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "involved-dealing",
   "metadata": {},
   "source": [
    "**Accuracy Score from Kaggle = 84.741%**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norman-witch",
   "metadata": {},
   "source": [
    "### For top 10 Covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "registered-defendant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:12:38] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "CPU times: user 2min 9s, sys: 750 ms, total: 2min 10s\n",
      "Wall time: 38.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=42,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# First 300K rows is our training set.\n",
    "top10_train_X_data = find_one_encoder(training_10.copy(),testing_10.copy()).iloc[:300000,:]\n",
    "# Last 200K rows is our testing set.\n",
    "top10_test_X_data = find_one_encoder(training_10.copy(),testing_10.copy()).iloc[300000:,:]\n",
    " \n",
    "# Constructing the XGBClassifier \n",
    "xgb_model = XGBClassifier(random_state = 42)\n",
    "# Fitting the XGBClassifier on full dataset.\n",
    "xgb_model.fit(top10_train_X_data, trainY_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "twenty-liability",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting values using the test X matrix\n",
    "y_predicted = xgb_model.predict_proba(top10_test_X_data)[:,1]\n",
    "# Calling function to write the csv file\n",
    "submit_file['target'] = y_predicted\n",
    "submit_file.to_csv('top10-xgboost.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composed-anaheim",
   "metadata": {},
   "source": [
    "**Accuracy Score from Kaggle = 85.562%**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "departmental-mambo",
   "metadata": {},
   "source": [
    "### For top 15 Covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "monetary-computer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:14:20] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "CPU times: user 18min 25s, sys: 4.42 s, total: 18min 29s\n",
      "Wall time: 5min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=42,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# First 300K rows is our training set.\n",
    "top15_train_X_data = find_one_encoder(training_15.copy(),testing_15.copy()).iloc[:300000,:]\n",
    "# Last 200K rows is our testing set.\n",
    "top15_test_X_data = find_one_encoder(training_15.copy(),testing_15.copy()).iloc[300000:,:]\n",
    " \n",
    "# Constructing the XGBClassifier \n",
    "xgb_model = XGBClassifier(random_state = 42)\n",
    "# Fitting the XGBClassifier on full dataset.\n",
    "xgb_model.fit(top15_train_X_data, trainY_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "extra-microwave",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting values using the test X matrix\n",
    "y_predicted = xgb_model.predict_proba(top15_test_X_data)[:,1]\n",
    "# Calling function to write the csv file\n",
    "submit_file['target'] = y_predicted\n",
    "submit_file.to_csv('top15-xgboost.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numeric-involvement",
   "metadata": {},
   "source": [
    "**Accuracy Score from Kaggle = 86.683%**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welsh-antigua",
   "metadata": {},
   "source": [
    "### For top 20 Covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "english-insulation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:20:19] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "CPU times: user 20min 6s, sys: 4.8 s, total: 20min 10s\n",
      "Wall time: 5min 25s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=42,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# First 300K rows is our training set.\n",
    "top20_train_X_data = find_one_encoder(training_20.copy(),testing_20.copy()).iloc[:300000,:]\n",
    "# Last 200K rows is our testing set.\n",
    "top20_test_X_data = find_one_encoder(training_20.copy(),testing_20.copy()).iloc[300000:,:]\n",
    " \n",
    "# Constructing the XGBClassifier \n",
    "xgb_model = XGBClassifier(random_state = 42)\n",
    "# Fitting the XGBClassifier on full dataset.\n",
    "xgb_model.fit(top20_train_X_data, trainY_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "alternate-baltimore",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting values using the test X matrix\n",
    "y_predicted = xgb_model.predict_proba(top20_test_X_data)[:,1]\n",
    "# Calling function to write the csv file\n",
    "submit_file['target'] = y_predicted\n",
    "submit_file.to_csv('top20-xgboost.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-nylon",
   "metadata": {},
   "source": [
    "**Accuracy Score from Kaggle = 88.104%**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proper-question",
   "metadata": {},
   "source": [
    "### For top 25 Covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "stopped-mentor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:26:24] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "CPU times: user 22min 20s, sys: 8.12 s, total: 22min 28s\n",
      "Wall time: 6min 37s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=42,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# First 300K rows is our training set.\n",
    "top25_train_X_data = find_one_encoder(training_25.copy(),testing_25.copy()).iloc[:300000,:]\n",
    "# Last 200K rows is our testing set.\n",
    "top25_test_X_data = find_one_encoder(training_25.copy(),testing_25.copy()).iloc[300000:,:]\n",
    " \n",
    "# Constructing the XGBClassifier \n",
    "xgb_model = XGBClassifier(random_state = 42)\n",
    "# Fitting the XGBClassifier on full dataset.\n",
    "xgb_model.fit(top25_train_X_data, trainY_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "shared-skirt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting values using the test X matrix\n",
    "y_predicted = xgb_model.predict_proba(top25_test_X_data)[:,1]\n",
    "# Calling function to write the csv file\n",
    "submit_file['target'] = y_predicted\n",
    "submit_file.to_csv('top25-xgboost.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-communist",
   "metadata": {},
   "source": [
    "**Accuracy Score from Kaggle = 88.357%**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fixed-hobby",
   "metadata": {},
   "source": [
    "### For top 30 Covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italic-detail",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# Constructing the XGBClassifier \n",
    "xgb_model = XGBClassifier(random_state = 42)\n",
    "# Fitting the XGBClassifier on full dataset.\n",
    "xgb_model.fit(full_train_X_data, trainY_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frozen-miami",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting values using the test X matrix\n",
    "y_predicted = xgb_model.predict_proba(full_test_X_data)[:,1]\n",
    "# Calling function to write the csv file\n",
    "submit_file['target'] = y_predicted\n",
    "submit_file.to_csv('top30-xgboost.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupied-algebra",
   "metadata": {},
   "source": [
    "**Accuracy Score from Kaggle = 88.564%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "amber-guidance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Putting results in data frame to plot the trade-off.\n",
    "data = {'Number Of Covariates':  [5, 10, 15, 20, 25, 30],\n",
    "        'Accuracy Score': [84.741, 85.562, 86.683, 88.104, 88.357,88.564]\n",
    "        }\n",
    "\n",
    "trade_off = pd.DataFrame (data, columns = ['Number Of Covariates','Accuracy Score'])\n",
    "\n",
    "# Plotting data\n",
    "trade_off_plot = sns.lineplot(data=trade_off, x=\"Number Of Covariates\", y=\"Accuracy Score\")\n",
    "trade_off_plot.set_title(\"Number covariate-accuracy tradeoff\")\n",
    "# Saving plot\n",
    "figure_save = trade_off_plot.get_figure()\n",
    "figure_save.savefig(\"trade-off.png\")\n",
    "\n",
    "# Previewing plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
