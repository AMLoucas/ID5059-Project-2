{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "behind-merit",
   "metadata": {},
   "source": [
    "# Classification Assesment Technical Report\n",
    "\n",
    "\n",
    "### Before running code you must :\n",
    "\n",
    " * Make sure to have all data being used in the appropriate path(data/..)\n",
    " * Using the correct python version (>3) \n",
    " * Make sure to have all the libraries being used installed on your machine.\n",
    "\n",
    "### Workflow being implemented for the assesment :\n",
    "        \n",
    "  1. Import data in the enviroment\n",
    "  2. Exploration analysis and summary statistics\n",
    "  3. Fitting 3 models to assess begininng state\n",
    "  4. Overfit/Underfit trade-off of best perfoming model\n",
    "  5. Feature importance on the best perfoming model and reduce complexity (overfit)\n",
    "  6. Tuning hyperparameters of best performing model\n",
    "  7. Results of the best perfoming model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innovative-honduras",
   "metadata": {},
   "source": [
    "###### [1] Importing the dataset in python enviroment\n",
    "\n",
    "We are also importing all the appropriate libraries that will be used in the following process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "international-pressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important libraries for data frame manipulations.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys as sys\n",
    "\n",
    "# For fitting a decision tree.\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# For fitting logistic regression.\n",
    "\n",
    "# Any others needed, feel free to append.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "affiliated-yukon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the training and testing set already provided.\n",
    "training_data = pd.read_csv(\"data/train.csv\")\n",
    "testing_data = pd.read_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checked-advice",
   "metadata": {},
   "source": [
    "###### [2] Exploration Analysis applied on the data\n",
    "\n",
    "This step is fundamental for the construction of our candidate classifiers. We have to make sure that our two datasets have the same strucuture (covariates). However this step is undertaken with extreme caution, because the test data should not be viewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "stretch-mounting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 300000 entries, 0 to 299999\n",
      "Data columns (total 32 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   id      300000 non-null  int64  \n",
      " 1   cat0    300000 non-null  object \n",
      " 2   cat1    300000 non-null  object \n",
      " 3   cat2    300000 non-null  object \n",
      " 4   cat3    300000 non-null  object \n",
      " 5   cat4    300000 non-null  object \n",
      " 6   cat5    300000 non-null  object \n",
      " 7   cat6    300000 non-null  object \n",
      " 8   cat7    300000 non-null  object \n",
      " 9   cat8    300000 non-null  object \n",
      " 10  cat9    300000 non-null  object \n",
      " 11  cat10   300000 non-null  object \n",
      " 12  cat11   300000 non-null  object \n",
      " 13  cat12   300000 non-null  object \n",
      " 14  cat13   300000 non-null  object \n",
      " 15  cat14   300000 non-null  object \n",
      " 16  cat15   300000 non-null  object \n",
      " 17  cat16   300000 non-null  object \n",
      " 18  cat17   300000 non-null  object \n",
      " 19  cat18   300000 non-null  object \n",
      " 20  cont0   300000 non-null  float64\n",
      " 21  cont1   300000 non-null  float64\n",
      " 22  cont2   300000 non-null  float64\n",
      " 23  cont3   300000 non-null  float64\n",
      " 24  cont4   300000 non-null  float64\n",
      " 25  cont5   300000 non-null  float64\n",
      " 26  cont6   300000 non-null  float64\n",
      " 27  cont7   300000 non-null  float64\n",
      " 28  cont8   300000 non-null  float64\n",
      " 29  cont9   300000 non-null  float64\n",
      " 30  cont10  300000 non-null  float64\n",
      " 31  target  300000 non-null  int64  \n",
      "dtypes: float64(11), int64(2), object(19)\n",
      "memory usage: 73.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# Having info analysis on the training dataset.\n",
    "training_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "contained-literature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200000 entries, 0 to 199999\n",
      "Data columns (total 31 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   id      200000 non-null  int64  \n",
      " 1   cat0    200000 non-null  object \n",
      " 2   cat1    200000 non-null  object \n",
      " 3   cat2    200000 non-null  object \n",
      " 4   cat3    200000 non-null  object \n",
      " 5   cat4    200000 non-null  object \n",
      " 6   cat5    200000 non-null  object \n",
      " 7   cat6    200000 non-null  object \n",
      " 8   cat7    200000 non-null  object \n",
      " 9   cat8    200000 non-null  object \n",
      " 10  cat9    200000 non-null  object \n",
      " 11  cat10   200000 non-null  object \n",
      " 12  cat11   200000 non-null  object \n",
      " 13  cat12   200000 non-null  object \n",
      " 14  cat13   200000 non-null  object \n",
      " 15  cat14   200000 non-null  object \n",
      " 16  cat15   200000 non-null  object \n",
      " 17  cat16   200000 non-null  object \n",
      " 18  cat17   200000 non-null  object \n",
      " 19  cat18   200000 non-null  object \n",
      " 20  cont0   200000 non-null  float64\n",
      " 21  cont1   200000 non-null  float64\n",
      " 22  cont2   200000 non-null  float64\n",
      " 23  cont3   200000 non-null  float64\n",
      " 24  cont4   200000 non-null  float64\n",
      " 25  cont5   200000 non-null  float64\n",
      " 26  cont6   200000 non-null  float64\n",
      " 27  cont7   200000 non-null  float64\n",
      " 28  cont8   200000 non-null  float64\n",
      " 29  cont9   200000 non-null  float64\n",
      " 30  cont10  200000 non-null  float64\n",
      "dtypes: float64(11), int64(1), object(19)\n",
      "memory usage: 47.3+ MB\n"
     ]
    }
   ],
   "source": [
    "# Having info analysis on the testing dataset\n",
    "testing_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adolescent-force",
   "metadata": {},
   "source": [
    "From the above results, we confirm that the two datasets do not hold any null values and they are of the same design structure.\n",
    "\n",
    " * Both datasets contain 19 factor type covariates\n",
    " * Both datasets contain 11 float type covariates\n",
    " * The training dataset has 1 extra column of type int which is our response variable of training\n",
    " * Training set is constructed by 300K rows\n",
    " * Testing set is constructed by 200K rows\n",
    " \n",
    "We now check the different levels of each factor variable. The reason behind this action is because in Python our testing and training set must have the same number of factor levels for the identical covariate. When we apply 1-hot-encoder, each level will act as a unique covariate. With this being said if one factor covariate has different number levels between the two sets, the design structure wont be appropriate.\n",
    "\n",
    "**1-Hot-Encoder** is a method used in Python to overcome the different levels of the categorical (string) columns of the data. Because Python models only understand numerical values, we need to convert the string data to numerical without implementing a mathematical meaning to them. This technique creates a new column for **each** different level and filles in the cells with **binary values** (1 = true, 0 = false). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "patent-reducing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat0</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>cat10</th>\n",
       "      <th>cat11</th>\n",
       "      <th>cat12</th>\n",
       "      <th>cat13</th>\n",
       "      <th>cat14</th>\n",
       "      <th>cat15</th>\n",
       "      <th>cat16</th>\n",
       "      <th>cat17</th>\n",
       "      <th>cat18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>300000</td>\n",
       "      <td>300000</td>\n",
       "      <td>300000</td>\n",
       "      <td>300000</td>\n",
       "      <td>300000</td>\n",
       "      <td>300000</td>\n",
       "      <td>300000</td>\n",
       "      <td>300000</td>\n",
       "      <td>300000</td>\n",
       "      <td>300000</td>\n",
       "      <td>300000</td>\n",
       "      <td>300000</td>\n",
       "      <td>300000</td>\n",
       "      <td>300000</td>\n",
       "      <td>300000</td>\n",
       "      <td>300000</td>\n",
       "      <td>300000</td>\n",
       "      <td>300000</td>\n",
       "      <td>300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>13</td>\n",
       "      <td>20</td>\n",
       "      <td>84</td>\n",
       "      <td>16</td>\n",
       "      <td>51</td>\n",
       "      <td>61</td>\n",
       "      <td>19</td>\n",
       "      <td>299</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>A</td>\n",
       "      <td>I</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>BI</td>\n",
       "      <td>A</td>\n",
       "      <td>AH</td>\n",
       "      <td>BM</td>\n",
       "      <td>A</td>\n",
       "      <td>DJ</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>223525</td>\n",
       "      <td>90809</td>\n",
       "      <td>168694</td>\n",
       "      <td>187251</td>\n",
       "      <td>129385</td>\n",
       "      <td>238563</td>\n",
       "      <td>187896</td>\n",
       "      <td>45818</td>\n",
       "      <td>42380</td>\n",
       "      <td>201945</td>\n",
       "      <td>31584</td>\n",
       "      <td>258932</td>\n",
       "      <td>257139</td>\n",
       "      <td>292712</td>\n",
       "      <td>160166</td>\n",
       "      <td>203574</td>\n",
       "      <td>206906</td>\n",
       "      <td>247125</td>\n",
       "      <td>255482</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          cat0    cat1    cat2    cat3    cat4    cat5    cat6    cat7  \\\n",
       "count   300000  300000  300000  300000  300000  300000  300000  300000   \n",
       "unique       2      15      19      13      20      84      16      51   \n",
       "top          A       I       A       A       E      BI       A      AH   \n",
       "freq    223525   90809  168694  187251  129385  238563  187896   45818   \n",
       "\n",
       "          cat8    cat9   cat10   cat11   cat12   cat13   cat14   cat15  \\\n",
       "count   300000  300000  300000  300000  300000  300000  300000  300000   \n",
       "unique      61      19     299       2       2       2       2       4   \n",
       "top         BM       A      DJ       A       A       A       A       B   \n",
       "freq     42380  201945   31584  258932  257139  292712  160166  203574   \n",
       "\n",
       "         cat16   cat17   cat18  \n",
       "count   300000  300000  300000  \n",
       "unique       4       4       4  \n",
       "top          D       D       B  \n",
       "freq    206906  247125  255482  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using describe function to check the levels of the categories in training dataset.\n",
    "training_data.describe(include=[object])\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "turned-professor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat0</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>cat10</th>\n",
       "      <th>cat11</th>\n",
       "      <th>cat12</th>\n",
       "      <th>cat13</th>\n",
       "      <th>cat14</th>\n",
       "      <th>cat15</th>\n",
       "      <th>cat16</th>\n",
       "      <th>cat17</th>\n",
       "      <th>cat18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200000</td>\n",
       "      <td>200000</td>\n",
       "      <td>200000</td>\n",
       "      <td>200000</td>\n",
       "      <td>200000</td>\n",
       "      <td>200000</td>\n",
       "      <td>200000</td>\n",
       "      <td>200000</td>\n",
       "      <td>200000</td>\n",
       "      <td>200000</td>\n",
       "      <td>200000</td>\n",
       "      <td>200000</td>\n",
       "      <td>200000</td>\n",
       "      <td>200000</td>\n",
       "      <td>200000</td>\n",
       "      <td>200000</td>\n",
       "      <td>200000</td>\n",
       "      <td>200000</td>\n",
       "      <td>200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>13</td>\n",
       "      <td>20</td>\n",
       "      <td>84</td>\n",
       "      <td>16</td>\n",
       "      <td>51</td>\n",
       "      <td>61</td>\n",
       "      <td>19</td>\n",
       "      <td>295</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>A</td>\n",
       "      <td>I</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>BI</td>\n",
       "      <td>A</td>\n",
       "      <td>AH</td>\n",
       "      <td>BM</td>\n",
       "      <td>A</td>\n",
       "      <td>DJ</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>149023</td>\n",
       "      <td>60152</td>\n",
       "      <td>112465</td>\n",
       "      <td>124506</td>\n",
       "      <td>86073</td>\n",
       "      <td>158916</td>\n",
       "      <td>125098</td>\n",
       "      <td>30593</td>\n",
       "      <td>28368</td>\n",
       "      <td>134223</td>\n",
       "      <td>21166</td>\n",
       "      <td>172586</td>\n",
       "      <td>171098</td>\n",
       "      <td>195016</td>\n",
       "      <td>106607</td>\n",
       "      <td>135542</td>\n",
       "      <td>137908</td>\n",
       "      <td>165066</td>\n",
       "      <td>170068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          cat0    cat1    cat2    cat3    cat4    cat5    cat6    cat7  \\\n",
       "count   200000  200000  200000  200000  200000  200000  200000  200000   \n",
       "unique       2      15      19      13      20      84      16      51   \n",
       "top          A       I       A       A       E      BI       A      AH   \n",
       "freq    149023   60152  112465  124506   86073  158916  125098   30593   \n",
       "\n",
       "          cat8    cat9   cat10   cat11   cat12   cat13   cat14   cat15  \\\n",
       "count   200000  200000  200000  200000  200000  200000  200000  200000   \n",
       "unique      61      19     295       2       2       2       2       4   \n",
       "top         BM       A      DJ       A       A       A       A       B   \n",
       "freq     28368  134223   21166  172586  171098  195016  106607  135542   \n",
       "\n",
       "         cat16   cat17   cat18  \n",
       "count   200000  200000  200000  \n",
       "unique       4       4       4  \n",
       "top          D       D       B  \n",
       "freq    137908  165066  170068  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using describe function to check the levels of categories in testing dataset.\n",
    "testing_data.describe(include=[object])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immediate-investigation",
   "metadata": {},
   "source": [
    "From the above results we can view that in the testing data column 'cat10' has 4 extra unique values. For this reason we will need to 1-hot-encode the categorical data for both datasets together and split them again to the beginning states.\n",
    "\n",
    "Before we merge the two datasets and ecnode them, we need to isolate the response in the training dataset alone. Additionally we need to remove the index column from the data, because it should not be used in the training or predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "hispanic-crowd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a copy of data frames to 1-hot-encode\n",
    "\n",
    "## TRAINING DATA\n",
    "train_encoder = training_data.copy()\n",
    "# Need to drop response varibale from training and hold it seperate.\n",
    "trainY_response = training_data['target'].copy()\n",
    "train_encoder.drop('target', axis = 1, inplace = True)\n",
    "# We also need to remove the index variables because they should not be considered in the model training\n",
    "train_encoder.drop('id', axis = 1, inplace = True)\n",
    "\n",
    "## TESTING DATA\n",
    "test_encoder = testing_data.copy()\n",
    "test_encoder.drop('id', axis = 1, inplace = True)\n",
    "\n",
    "## MERGED DATA\n",
    "# This will concatenate the test data rows below the train data\n",
    "# The first 300K rows will be the train.\n",
    "# The last 200K rows will be the testing.\n",
    "merged_encoder = pd.concat([train_encoder, test_encoder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "breathing-guard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 5.96 µs\n"
     ]
    }
   ],
   "source": [
    "# Using get_dummies() which is the same thing as 1-hot-encoder but ignores numerical values.\n",
    "%time\n",
    "merged_encoder = pd.get_dummies(merged_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "rising-constraint",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 500000 entries, 0 to 199999\n",
      "Columns: 642 entries, cont0 to cat18_D\n",
      "dtypes: float64(11), uint8(631)\n",
      "memory usage: 346.7 MB\n"
     ]
    }
   ],
   "source": [
    "# Checking if the one hot encoder worked.\n",
    "merged_encoder.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iraqi-cartridge",
   "metadata": {},
   "source": [
    "From the above result we can understand that our data has become a large sparse matrix of values. This is one of the consequences when using a 1-Hot-Encoder technique. We now need to split the data again back to training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "swedish-hammer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 300000 entries, 0 to 299999\n",
      "Columns: 642 entries, cont0 to cat18_D\n",
      "dtypes: float64(11), uint8(631)\n",
      "memory usage: 208.0 MB\n"
     ]
    }
   ],
   "source": [
    "# First 300K rows is our training set.\n",
    "trainingX_data = merged_encoder.iloc[:300000,:].copy()\n",
    "trainingX_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "appointed-electric",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 200000 entries, 0 to 199999\n",
      "Columns: 642 entries, cont0 to cat18_D\n",
      "dtypes: float64(11), uint8(631)\n",
      "memory usage: 138.7 MB\n"
     ]
    }
   ],
   "source": [
    "# Last 200K rows is our testing set.\n",
    "testingX_data = merged_encoder.iloc[300000:,:].copy()\n",
    "testingX_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-terminology",
   "metadata": {},
   "source": [
    "We can now explore correlation relations between the covariates with response. This will help us identify the covariates that mostly influence the response value. The covariates that mostly influence the response values are also most probably the most important covariates to consider in our training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "outer-september",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.81 µs\n"
     ]
    }
   ],
   "source": [
    "# Merge the trainingX_data with trainY_data to see the correlations\n",
    "training_all = pd.concat([trainingX_data.copy(), trainY_response.copy()], axis=1)\n",
    "%time\n",
    "# Computing correlation relations. (takes about 5-6 minutes btw. Time below is worng)\n",
    "training_corr_influnce = training_all.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "embedded-amber",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target     1.000000\n",
       "cat16_B    0.522759\n",
       "cat15_D    0.467675\n",
       "cat14_B    0.302301\n",
       "cat18_D    0.299653\n",
       "cat11_B    0.285503\n",
       "cat0_A     0.268109\n",
       "cat18_C    0.260021\n",
       "cat17_C    0.237540\n",
       "cont5      0.215184\n",
       "cat2_Q     0.213173\n",
       "cat13_B    0.205714\n",
       "cat8_K     0.194427\n",
       "cat1_L     0.190920\n",
       "cont6      0.189832\n",
       "cont8      0.183726\n",
       "cont1      0.164655\n",
       "cat7_AF    0.160744\n",
       "cat9_A     0.156035\n",
       "cat4_H     0.153590\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing the correlation with response in sorted way\n",
    "training_corr_influnce[\"target\"].sort_values(ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordinary-spokesman",
   "metadata": {},
   "source": [
    "**One-Hot-Encoder** is completed now and **Correlation analysis of response** has also been applied as an indication of what features will be valuable in our training.\n",
    "A quick recap to our different data.frames we have to now.\n",
    "   \n",
    " 1. NON-One-Hot-Encoded-Data\n",
    " * training_data = Full training dataset to reference back to it if mistake occurs further on.\n",
    " * testing_data = Full testing dataset to reference back to it if mistake occures.\n",
    " 2. One-Hot-Encoded-Data\n",
    " * merged_encoder = Both datasets merged and encoded\n",
    " * trainingX_data = Training dataset explanatory variables. (values we will use for training)\n",
    " * trainY_response = Training dataset response variable. (value used to training models in construnction)\n",
    " * testingX_data = Testing dataser explanatory variables (values we will use for predictions.)\n",
    " 3. Extra infomration\n",
    " * training_corr_influnce = Holds correlation values between variables\n",
    " * training_all = Holds the 1-Hot-Encoded explanatory variables with the response, to calculate correlations.\n",
    " \n",
    "More exploration analysis and graph analysis can be found in the external file EDA.ipynp."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configured-despite",
   "metadata": {},
   "source": [
    "######  [3] Fitting 3 models to assess begininng state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungry-switch",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convenient-perception",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unauthorized-testimony",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expanded-bradley",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "gothic-cannon",
   "metadata": {},
   "source": [
    "###### [4] Overfit/Underfit trade-off of best perfoming mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acoustic-trouble",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beautiful-investing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personalized-scheduling",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "instructional-marriage",
   "metadata": {},
   "source": [
    "###### [5] Feature importance on the best perfoming model and reduce complexity (overfit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-cooperation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-dairy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otherwise-password",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "listed-planner",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ranging-commissioner",
   "metadata": {},
   "source": [
    "###### [6] Tuning hyperparameters of best performing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-beijing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sapphire-extent",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variable-chaos",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confirmed-arrow",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "driving-accountability",
   "metadata": {},
   "source": [
    "###### [7] Results of the best perfoming model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spanish-seven",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
